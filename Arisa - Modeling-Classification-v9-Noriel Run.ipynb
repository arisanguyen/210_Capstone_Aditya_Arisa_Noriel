{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eabcba-5ca5-4fa8-bde3-8224131c3a6a",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a9c63264-b3ca-4d09-8d90-8f228ea7a33f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting tensorflow==2.11\n",
      "  Downloading tensorflow-2.11.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (588.3 MB)\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m588.3/588.3 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hCollecting tensorflow-estimator<2.12,>=2.11.0\n",
      "  Using cached tensorflow_estimator-2.11.0-py2.py3-none-any.whl (439 kB)\n",
      "Requirement already satisfied: wrapt>=1.11.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.14.1)\n",
      "Requirement already satisfied: termcolor>=1.1.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (2.2.0)\n",
      "Requirement already satisfied: google-pasta>=0.1.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (0.2.0)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.16.0)\n",
      "Requirement already satisfied: absl-py>=1.0.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.4.0)\n",
      "Requirement already satisfied: setuptools in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (65.5.0)\n",
      "Requirement already satisfied: typing-extensions>=3.6.6 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (4.5.0)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (23.0)\n",
      "Requirement already satisfied: opt-einsum>=2.3.2 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (3.3.0)\n",
      "Requirement already satisfied: astunparse>=1.6.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.6.3)\n",
      "Collecting protobuf<3.20,>=3.9.2\n",
      "  Using cached protobuf-3.19.6-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.1 MB)\n",
      "Requirement already satisfied: h5py>=2.9.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (3.8.0)\n",
      "Requirement already satisfied: grpcio<2.0,>=1.24.3 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.53.0)\n",
      "Requirement already satisfied: numpy>=1.20 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (1.23.5)\n",
      "Requirement already satisfied: flatbuffers>=2.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (23.3.3)\n",
      "Collecting tensorboard<2.12,>=2.11\n",
      "  Using cached tensorboard-2.11.2-py3-none-any.whl (6.0 MB)\n",
      "Requirement already satisfied: gast<=0.4.0,>=0.2.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (0.4.0)\n",
      "Requirement already satisfied: tensorflow-io-gcs-filesystem>=0.23.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (0.32.0)\n",
      "Collecting keras<2.12,>=2.11.0\n",
      "  Using cached keras-2.11.0-py2.py3-none-any.whl (1.7 MB)\n",
      "Requirement already satisfied: libclang>=13.0.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorflow==2.11) (16.0.0)\n",
      "Requirement already satisfied: wheel<1.0,>=0.23.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from astunparse>=1.6.0->tensorflow==2.11) (0.40.0)\n",
      "Requirement already satisfied: werkzeug>=1.0.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.2.3)\n",
      "Collecting google-auth-oauthlib<0.5,>=0.4.1\n",
      "  Using cached google_auth_oauthlib-0.4.6-py2.py3-none-any.whl (18 kB)\n",
      "Requirement already satisfied: markdown>=2.6.8 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (3.4.3)\n",
      "Collecting tensorboard-data-server<0.7.0,>=0.6.0\n",
      "  Using cached tensorboard_data_server-0.6.1-py3-none-manylinux2010_x86_64.whl (4.9 MB)\n",
      "Requirement already satisfied: google-auth<3,>=1.6.3 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.17.0)\n",
      "Requirement already satisfied: tensorboard-plugin-wit>=1.6.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (1.8.1)\n",
      "Requirement already satisfied: requests<3,>=2.21.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tensorboard<2.12,>=2.11->tensorflow==2.11) (2.28.2)\n",
      "Requirement already satisfied: rsa<5,>=3.1.4 in /home/ubuntu/capstone/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (4.9)\n",
      "Requirement already satisfied: cachetools<6.0,>=2.0.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (5.3.0)\n",
      "Requirement already satisfied: pyasn1-modules>=0.2.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.2.8)\n",
      "Requirement already satisfied: requests-oauthlib>=0.7.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.3.1)\n",
      "Requirement already satisfied: certifi>=2017.4.17 in /home/ubuntu/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (2022.12.7)\n",
      "Requirement already satisfied: idna<4,>=2.5 in /home/ubuntu/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.4)\n",
      "Requirement already satisfied: urllib3<1.27,>=1.21.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (1.26.15)\n",
      "Requirement already satisfied: charset-normalizer<4,>=2 in /home/ubuntu/capstone/lib/python3.10/site-packages (from requests<3,>=2.21.0->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.1.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.1.1 in /home/ubuntu/capstone/lib/python3.10/site-packages (from werkzeug>=1.0.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (2.1.2)\n",
      "Requirement already satisfied: pyasn1<0.5.0,>=0.4.6 in /home/ubuntu/capstone/lib/python3.10/site-packages (from pyasn1-modules>=0.2.1->google-auth<3,>=1.6.3->tensorboard<2.12,>=2.11->tensorflow==2.11) (0.4.8)\n",
      "Requirement already satisfied: oauthlib>=3.0.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib<0.5,>=0.4.1->tensorboard<2.12,>=2.11->tensorflow==2.11) (3.2.2)\n",
      "Installing collected packages: tensorflow-estimator, tensorboard-data-server, protobuf, keras, google-auth-oauthlib, tensorboard, tensorflow\n",
      "  Attempting uninstall: tensorflow-estimator\n",
      "    Found existing installation: tensorflow-estimator 2.12.0\n",
      "    Uninstalling tensorflow-estimator-2.12.0:\n",
      "      Successfully uninstalled tensorflow-estimator-2.12.0\n",
      "  Attempting uninstall: tensorboard-data-server\n",
      "    Found existing installation: tensorboard-data-server 0.7.0\n",
      "    Uninstalling tensorboard-data-server-0.7.0:\n",
      "      Successfully uninstalled tensorboard-data-server-0.7.0\n",
      "  Attempting uninstall: protobuf\n",
      "    Found existing installation: protobuf 4.22.1\n",
      "    Uninstalling protobuf-4.22.1:\n",
      "      Successfully uninstalled protobuf-4.22.1\n",
      "  Attempting uninstall: keras\n",
      "    Found existing installation: keras 2.12.0\n",
      "    Uninstalling keras-2.12.0:\n",
      "      Successfully uninstalled keras-2.12.0\n",
      "  Attempting uninstall: google-auth-oauthlib\n",
      "    Found existing installation: google-auth-oauthlib 1.0.0\n",
      "    Uninstalling google-auth-oauthlib-1.0.0:\n",
      "      Successfully uninstalled google-auth-oauthlib-1.0.0\n",
      "  Attempting uninstall: tensorboard\n",
      "    Found existing installation: tensorboard 2.12.1\n",
      "    Uninstalling tensorboard-2.12.1:\n",
      "      Successfully uninstalled tensorboard-2.12.1\n",
      "  Attempting uninstall: tensorflow\n",
      "    Found existing installation: tensorflow 2.12.0\n",
      "    Uninstalling tensorflow-2.12.0:\n",
      "      Successfully uninstalled tensorflow-2.12.0\n",
      "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "tensorflow-decision-forests 1.3.0 requires tensorflow~=2.12.0, but you have tensorflow 2.11.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[0mSuccessfully installed google-auth-oauthlib-0.4.6 keras-2.11.0 protobuf-3.19.6 tensorboard-2.11.2 tensorboard-data-server-0.6.1 tensorflow-2.11.0 tensorflow-estimator-2.11.0\n",
      "Requirement already satisfied: tfa-nightly in /home/ubuntu/capstone/lib/python3.10/site-packages (0.20.0.dev20230301043356)\n",
      "Requirement already satisfied: typeguard>=2.7 in /home/ubuntu/capstone/lib/python3.10/site-packages (from tfa-nightly) (3.0.2)\n",
      "Requirement already satisfied: packaging in /home/ubuntu/capstone/lib/python3.10/site-packages (from tfa-nightly) (23.0)\n",
      "Requirement already satisfied: typing-extensions>=4.4.0 in /home/ubuntu/capstone/lib/python3.10/site-packages (from typeguard>=2.7->tfa-nightly) (4.5.0)\n"
     ]
    }
   ],
   "source": [
    "# !pip3 install numpy\n",
    "# !pip3 install pandas\n",
    "# !pip3 install matplotlib\n",
    "# !pip3 install tensorflow\n",
    "# !pip3 install tifffile\n",
    "# !pip3 install -U scikit-learn scipy matplotlib\n",
    "# !pip3 install tensorflow_decision_forests --upgrade\n",
    "# !pip install imagecodecs\n",
    "# !pip install --no-cache-dir tifffile \n",
    "# !pip install --no-cache-dir tensorflow_decision_forests\n",
    "#!pip install --upgrade tensorflow==2.11\n",
    "pip install tfa-nightly"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "77f70fee-47d4-4001-bfee-39b1fdc92b4b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.12.0'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tf.__version__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "ef629f34-bb26-46f3-8e3f-907c47780cb3",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Python 3.10.10\n"
     ]
    }
   ],
   "source": [
    "!python --version"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "356025b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-07 05:46:50.968965: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-04-07 05:46:53.072025: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'tensorflow_addons'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 13\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m layers\n\u001b[1;32m     12\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mtensorflow\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mlayers\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m concatenate\n\u001b[0;32m---> 13\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mtensorflow_addons\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mtfa\u001b[39;00m\n\u001b[1;32m     15\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcallbacks\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m ModelCheckpoint\n\u001b[1;32m     16\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01mkeras\u001b[39;00m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmodels\u001b[39;00m \u001b[38;5;28;01mimport\u001b[39;00m load_model\n",
      "\u001b[0;31mModuleNotFoundError\u001b[0m: No module named 'tensorflow_addons'"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "import seaborn as sn\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import tensorflow_addons as tfa\n",
    "\n",
    "from keras.callbacks import ModelCheckpoint\n",
    "from keras.models import load_model\n",
    "\n",
    "import sklearn\n",
    "\n",
    "import tifffile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer, normalize\n",
    "\n",
    "\n",
    "import os\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b166dc2-360e-4118-9f65-0996bbc163a1",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "51c1eeab-df74-4a28-8b0f-7d937d42028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='train_loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "   # plt.plot(history.history['accuracy'], label='accuracy')\n",
    "   # plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['loss']),\n",
    "                max(history.history['val_loss'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "6d27acab-1145-4120-9a1c-c7da15f505d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "   # plt.plot(history.history['loss'], label='loss')\n",
    "   # plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(history.history['accuracy'], label='train_accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['accuracy']),\n",
    "                max(history.history['val_accuracy'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "c28e4eda-dfba-490b-9fd5-8f6f6cf61454",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_f1_macro(history):\n",
    "    plt.plot(history.history['f1_macro'], label='train_f1')\n",
    "    plt.plot(history.history['val_f1_macro'], label='val_f1')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['f1_macro']),\n",
    "                max(history.history['val_f1_macro'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Macro F1 Score')\n",
    "    plt.title('Macro F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "78d60b67-24b2-4399-bbed-8bdfa6687f77",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_f1_weighted(history):\n",
    "    plt.plot(history.history['f1_weighted'], label='train_f1')\n",
    "    plt.plot(history.history['val_f1_weighted'], label='val_f1')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['f1_weighted']),\n",
    "                max(history.history['val_f1_weighted'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Weighted F1 Score')\n",
    "    plt.title('Weighted F1 Score')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "dd28b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://github.com/jimmyyhwu/resnet18-tf2/blob/master/resnet.py\n",
    "\n",
    "kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n",
    "\n",
    "def conv3x3(x, out_planes, stride=1, name=None):\n",
    "    x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n",
    "    return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n",
    "\n",
    "def basic_block(x, planes, stride=1, downsample=None, name=None):\n",
    "    identity = x\n",
    "\n",
    "    out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n",
    "    out = layers.ReLU(name=f'{name}.relu1')(out)\n",
    "\n",
    "    out = conv3x3(out, planes, name=f'{name}.conv2')\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n",
    "\n",
    "    if downsample is not None:\n",
    "        for layer in downsample:\n",
    "            identity = layer(identity)\n",
    "\n",
    "    out = layers.Add(name=f'{name}.add')([identity, out])\n",
    "    out = layers.ReLU(name=f'{name}.relu2')(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def make_layer(x, planes, blocks, stride=1, name=None):\n",
    "    downsample = None\n",
    "    inplanes = x.shape[3]\n",
    "    if stride != 1 or inplanes != planes:\n",
    "        downsample = [\n",
    "            layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n",
    "        ]\n",
    "\n",
    "    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n",
    "    for i in range(1, blocks):\n",
    "        x = basic_block(x, planes, name=f'{name}.{i}')\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet(x, blocks_per_layer, num_classes=1000):\n",
    "    x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n",
    "    x = layers.ReLU(name='relu1')(x)\n",
    "    x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n",
    "\n",
    "    x1 = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n",
    "    x2 = make_layer(x1, 128, blocks_per_layer[1], stride=2, name='layer2')\n",
    "    x3 = make_layer(x2, 256, blocks_per_layer[2], stride=2, name='layer3')\n",
    "    x4 = make_layer(x3, 512, blocks_per_layer[3], stride=2, name='layer4')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name='avgpool')(x4)\n",
    "    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n",
    "    x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet18(x, **kwargs):\n",
    "    return resnet(x, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet34(x, **kwargs):\n",
    "    return resnet(x, [3, 4, 6, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a5b49-a808-4d41-ad4b-ae9469b994ea",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6b82c",
   "metadata": {},
   "source": [
    "Pulling in Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "0d50fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "tiles_train = pd.read_csv(r'./Data/Tiles_TRAIN.csv')\n",
    "#x_train = tiles_train.drop(['bins_numeric'], axis = 1)\n",
    "#y_train = tiles_train['bins_numeric']\n",
    "#x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "id": "6453b058-407b-4097-a463-b18eb8ebb124",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile_ID</th>\n",
       "      <th>Stop_Signs</th>\n",
       "      <th>Paving_historical</th>\n",
       "      <th>Bus_stop</th>\n",
       "      <th>Collisions_Historical</th>\n",
       "      <th>RTTYP_I</th>\n",
       "      <th>RTTYP_M</th>\n",
       "      <th>RTTYP_O</th>\n",
       "      <th>RTTYP_S</th>\n",
       "      <th>RTTYP_U</th>\n",
       "      <th>...</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94141</th>\n",
       "      <th>94143</th>\n",
       "      <th>94158</th>\n",
       "      <th>94188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1762</th>\n",
       "      <td>1526</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4603</th>\n",
       "      <td>11426</td>\n",
       "      <td>0.0</td>\n",
       "      <td>7.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>8.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6484</th>\n",
       "      <td>13993</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6950</th>\n",
       "      <td>3201</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4254</th>\n",
       "      <td>13779</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tile_ID  Stop_Signs  Paving_historical  Bus_stop  Collisions_Historical  \\\n",
       "1762     1526         0.0                0.0       0.0                    0.0   \n",
       "4603    11426         0.0                7.0       0.0                    8.0   \n",
       "6484    13993         0.0                0.0       0.0                    0.0   \n",
       "6950     3201         0.0                0.0       0.0                    0.0   \n",
       "4254    13779         0.0                0.0       0.0                    0.0   \n",
       "\n",
       "      RTTYP_I  RTTYP_M  RTTYP_O  RTTYP_S  RTTYP_U  ...  94129  94130  94131  \\\n",
       "1762        0        1        0        0        0  ...      0      0      0   \n",
       "4603        0        1        0        0        0  ...      0      0      0   \n",
       "6484        0        1        0        0        0  ...      0      0      0   \n",
       "6950        0        1        0        0        0  ...      0      0      0   \n",
       "4254        0        1        0        0        0  ...      0      0      0   \n",
       "\n",
       "      94132  94133  94134  94141  94143  94158  94188  \n",
       "1762      0      0      0      0      0      0      0  \n",
       "4603      0      0      0      0      0      0      0  \n",
       "6484      0      0      0      0      0      0      1  \n",
       "6950      1      0      0      0      0      0      0  \n",
       "4254      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_train, x_val, y_train, y_val = train_test_split(tiles_train.drop(['bins_numeric'], axis = 1), tiles_train['bins_numeric'],\n",
    "                                   random_state=104, \n",
    "                                   test_size=0.20, \n",
    "                                   shuffle=True)\n",
    "x_train.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "id": "5971c10e-9c78-4cd5-bf77-a213f4e70ead",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile_ID</th>\n",
       "      <th>Stop_Signs</th>\n",
       "      <th>Paving_historical</th>\n",
       "      <th>Bus_stop</th>\n",
       "      <th>Collisions_Historical</th>\n",
       "      <th>RTTYP_I</th>\n",
       "      <th>RTTYP_M</th>\n",
       "      <th>RTTYP_O</th>\n",
       "      <th>RTTYP_S</th>\n",
       "      <th>RTTYP_U</th>\n",
       "      <th>...</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94141</th>\n",
       "      <th>94143</th>\n",
       "      <th>94158</th>\n",
       "      <th>94188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>2732</th>\n",
       "      <td>2948</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>630</th>\n",
       "      <td>2945</td>\n",
       "      <td>4.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2887</th>\n",
       "      <td>7123</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>752</th>\n",
       "      <td>12171</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>2.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8106</th>\n",
       "      <td>1096</td>\n",
       "      <td>2.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 40 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      Tile_ID  Stop_Signs  Paving_historical  Bus_stop  Collisions_Historical  \\\n",
       "2732     2948         0.0                0.0       0.0                    0.0   \n",
       "630      2945         4.0                0.0       2.0                    1.0   \n",
       "2887     7123         1.0                0.0       0.0                    0.0   \n",
       "752     12171         0.0                0.0       2.0                    1.0   \n",
       "8106     1096         2.0                0.0       0.0                    0.0   \n",
       "\n",
       "      RTTYP_I  RTTYP_M  RTTYP_O  RTTYP_S  RTTYP_U  ...  94129  94130  94131  \\\n",
       "2732        0        1        0        0        0  ...      0      0      0   \n",
       "630         0        1        0        0        0  ...      0      0      0   \n",
       "2887        0        1        0        0        0  ...      0      0      0   \n",
       "752         0        1        0        0        0  ...      0      0      0   \n",
       "8106        0        1        0        0        0  ...      0      0      0   \n",
       "\n",
       "      94132  94133  94134  94141  94143  94158  94188  \n",
       "2732      0      0      0      0      0      0      0  \n",
       "630       0      0      0      0      0      0      0  \n",
       "2887      0      0      0      0      0      0      0  \n",
       "752       0      0      0      0      0      0      0  \n",
       "8106      0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 40 columns]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_val.head(5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "id": "d18a322c-14fa-44a5-89ec-8b6ae7411008",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Stop_Signs</th>\n",
       "      <th>Paving_historical</th>\n",
       "      <th>Bus_stop</th>\n",
       "      <th>Collisions_Historical</th>\n",
       "      <th>RTTYP_I</th>\n",
       "      <th>RTTYP_M</th>\n",
       "      <th>RTTYP_O</th>\n",
       "      <th>RTTYP_S</th>\n",
       "      <th>RTTYP_U</th>\n",
       "      <th>94101</th>\n",
       "      <th>...</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94141</th>\n",
       "      <th>94143</th>\n",
       "      <th>94158</th>\n",
       "      <th>94188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>3.0</td>\n",
       "      <td>6.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 39 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Stop_Signs  Paving_historical  Bus_stop  Collisions_Historical  RTTYP_I  \\\n",
       "0         0.0                0.0       0.0                    0.0        0   \n",
       "1         1.0                0.0       3.0                    6.0        0   \n",
       "2         0.0                0.0       0.0                    0.0        0   \n",
       "3         0.0                0.0       0.0                    0.0        0   \n",
       "4         0.0                0.0       0.0                    0.0        0   \n",
       "\n",
       "   RTTYP_M  RTTYP_O  RTTYP_S  RTTYP_U  94101  ...  94129  94130  94131  94132  \\\n",
       "0        1        0        0        0      0  ...      0      0      0      1   \n",
       "1        1        0        0        0      0  ...      0      0      0      0   \n",
       "2        1        0        0        0      0  ...      0      0      0      0   \n",
       "3        1        0        0        0      0  ...      0      1      0      0   \n",
       "4        1        0        0        0      0  ...      0      0      0      0   \n",
       "\n",
       "   94133  94134  94141  94143  94158  94188  \n",
       "0      0      0      0      0      0      0  \n",
       "1      0      0      0      0      0      0  \n",
       "2      0      0      0      0      0      0  \n",
       "3      0      0      0      0      0      0  \n",
       "4      0      1      0      0      0      0  \n",
       "\n",
       "[5 rows x 39 columns]"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles_test = pd.read_csv(r'./Data/Tiles_TEST.csv')\n",
    "x_test = tiles_test.drop(['Tile_ID','bins_numeric'], axis = 1)\n",
    "y_test = tiles_test['bins_numeric']\n",
    "x_test.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf220283",
   "metadata": {},
   "source": [
    "Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "id": "9298e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TRAIN SET \n",
    "\n",
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part1(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    \n",
    "    for id in x_train['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]/255\n",
    "            \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "    \n",
    "    return images_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "id": "4d8f77ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 148, 188, 4)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini = preprocess_data_part1(IMAGE_PATH)\n",
    "np.shape(images_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "id": "51e4bbbe-914b-47dd-9e82-93722404f70b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train = x_train.drop(['Tile_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "id": "84e1f01e-fc42-4bb8-9d18-e40fe8d215c6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# VAL SET \n",
    "\n",
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part15(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    \n",
    "    for id in x_val['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]/255\n",
    "            \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "#     images_mini = normalize(images_mini.reshape(6700,-1))\n",
    "    \n",
    "    return images_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "id": "1fcfd6d0-bf6d-48b1-a404-00afbc2b9633",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 148, 188, 4)"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini_v = preprocess_data_part15(IMAGE_PATH)\n",
    "np.shape(images_mini_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "96c499c7-9ec4-4f6c-8e9c-482de86f536b",
   "metadata": {},
   "outputs": [],
   "source": [
    "x_val = x_val.drop(['Tile_ID'], axis = 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "23790420-0c98-426f-8580-23f7d96e9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "# TEST SET\n",
    "\n",
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part2(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    \n",
    "    for id in tiles_test['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]/255\n",
    "            \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "    \n",
    "    return images_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "id": "cc8e3f65-2368-4092-9b09-ecda4e0a35d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095, 148, 188, 4)"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini_t = preprocess_data_part2(IMAGE_PATH)\n",
    "np.shape(images_mini_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57135842",
   "metadata": {},
   "source": [
    "Street Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "id": "8dce0686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 1, 1, 39)"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TRAIN SET \n",
    "\n",
    "street = np.asarray(x_train).astype('float32')\n",
    "street_mini = []\n",
    "for row in range(len(street)):\n",
    "    street_mini.append([[street[row]]])\n",
    "street_mini = np.stack(street_mini)\n",
    "np.shape(street_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "id": "c923520c-ed3c-4165-abd9-7b3cc932b35c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 1, 1, 39)"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# VAL SET \n",
    "\n",
    "street_v = np.asarray(x_val).astype('float32')\n",
    "street_mini_v = []\n",
    "for row in range(len(street_v)):\n",
    "    street_mini_v.append([[street_v[row]]])\n",
    "street_mini_v = np.stack(street_mini_v)\n",
    "np.shape(street_mini_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "id": "c28b0428-174b-4e10-b151-1354cf44bf79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095, 1, 1, 39)"
      ]
     },
     "execution_count": 56,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# TEST SET \n",
    "\n",
    "street_t = np.asarray(x_test).astype('float32')\n",
    "street_mini_t = []\n",
    "for row in range(len(street_t)):\n",
    "    street_mini_t.append([[street_t[row]]])\n",
    "street_mini_t = np.stack(street_mini_t)\n",
    "np.shape(street_mini_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec0dbb-2532-4c3a-b053-1f6f687be79c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f8956",
   "metadata": {},
   "source": [
    "Helpful Links: <br>\n",
    "https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ <br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50 <br>\n",
    "https://github.com/jimmyyhwu/resnet18-tf2/blob/master/resnet.py <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "020edd3a-0c97-45f9-875d-3df38db9251f",
   "metadata": {},
   "source": [
    "**Model 2.9.0: Simplified CNN**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "id": "af93dd1e-4394-4ba2-ac1c-593d00d2bff2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "      ones\n",
      "bins      \n",
      "0     5297\n",
      "1      108\n",
      "2      968\n",
      "3      230\n",
      "4       22\n",
      "5        4\n",
      "6       53\n",
      "7       11\n",
      "8        3\n",
      "9        1\n",
      "10       3\n"
     ]
    }
   ],
   "source": [
    "check = pd.DataFrame({'bins': y_train, 'ones': np.ones(len(y_train))}, columns=['bins', 'ones'])\n",
    "pivot = check.pivot_table(index=['bins'], values=['ones'], aggfunc=len)\n",
    "print(pivot)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "id": "42e82273-164e-45f2-86f3-5c7de058c9f6",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{0: 0.11498790052688486,\n",
       " 1: 5.63973063973064,\n",
       " 2: 0.6292261457550714,\n",
       " 3: 2.6482213438735176,\n",
       " 4: 27.68595041322314,\n",
       " 5: 152.27272727272728,\n",
       " 6: 11.492281303602057,\n",
       " 7: 55.37190082644628,\n",
       " 8: 203.03030303030303,\n",
       " 9: 609.0909090909091,\n",
       " 10: 203.03030303030303}"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.utils import class_weight \n",
    "class_weights = class_weight.compute_class_weight('balanced',classes = np.unique(y_train),y = y_train)\n",
    "class_weights = dict(enumerate(class_weights))\n",
    "class_weights"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "id": "6d656426-1f37-4a04-a8fe-f3f08160cf51",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6700, 50, 50, 39)"
      ]
     },
     "execution_count": 59,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_mini_2 = [] \n",
    "for k in range(len(street)):\n",
    "    for i in range(50): \n",
    "        for j in range(50):\n",
    "            street_mini_2.append(street[k])\n",
    "street_mini_2 = np.reshape(street_mini_2, (len(street),50,50,39))\n",
    "np.shape(street_mini_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "id": "b9cd3b05-a13a-4f51-be78-5831b546deb9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1676, 50, 50, 39)"
      ]
     },
     "execution_count": 60,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_mini_2_v = [] \n",
    "for k in range(len(street_v)):\n",
    "    for i in range(50): \n",
    "        for j in range(50):\n",
    "            street_mini_2_v.append(street_v[k])\n",
    "street_mini_2_v = np.reshape(street_mini_2_v, (len(street_v),50,50,39))\n",
    "np.shape(street_mini_2_v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "af91c34f-cb4c-47c3-bb19-4540b41de68a",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 21:27:38.127658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"model_2\"\n",
      "__________________________________________________________________________________________________\n",
      " Layer (type)                   Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      " Input_Images (InputLayer)      [(None, 148, 188, 4  0           []                               \n",
      "                                )]                                                                \n",
      "                                                                                                  \n",
      " conv2d (Conv2D)                (None, 146, 186, 6)  222         ['Input_Images[0][0]']           \n",
      "                                                                                                  \n",
      " Input_Street (InputLayer)      [(None, 50, 50, 39)  0           []                               \n",
      "                                ]                                                                 \n",
      "                                                                                                  \n",
      " max_pooling2d (MaxPooling2D)   (None, 48, 48, 6)    0           ['conv2d[0][0]']                 \n",
      "                                                                                                  \n",
      " conv2d_1 (Conv2D)              (None, 48, 48, 2)    704         ['Input_Street[0][0]']           \n",
      "                                                                                                  \n",
      " concatenate (Concatenate)      (None, 48, 48, 8)    0           ['max_pooling2d[0][0]',          \n",
      "                                                                  'conv2d_1[0][0]']               \n",
      "                                                                                                  \n",
      " conv2d_2 (Conv2D)              (None, 47, 47, 39)   1287        ['concatenate[0][0]']            \n",
      "                                                                                                  \n",
      " avgpool (GlobalAveragePooling2  (None, 39)          0           ['conv2d_2[0][0]']               \n",
      " D)                                                                                               \n",
      "                                                                                                  \n",
      " output (Dense)                 (None, 11)           440         ['avgpool[0][0]']                \n",
      "                                                                                                  \n",
      "==================================================================================================\n",
      "Total params: 2,653\n",
      "Trainable params: 2,653\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-04-06 21:27:38.526379: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:38.526658: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:38.528334: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:38.528556: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:38.528753: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:40.587161: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:40.587420: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:40.587681: I tensorflow/compiler/xla/stream_executor/cuda/cuda_gpu_executor.cc:996] successful NUMA node read from SysFS had negative value (-1), but there must be at least one NUMA node, so returning NUMA node zero. See more at https://github.com/torvalds/linux/blob/v6.0/Documentation/ABI/testing/sysfs-bus-pci#L344-L355\n",
      "2023-04-06 21:27:40.587848: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1635] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 14576 MB memory:  -> device: 0, name: Tesla V100-SXM2-16GB, pci bus id: 0000:00:1e.0, compute capability: 7.0\n"
     ]
    },
    {
     "ename": "NameError",
     "evalue": "name 'tfa' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[61], line 46\u001b[0m\n\u001b[1;32m     42\u001b[0m cnn_model \u001b[38;5;241m=\u001b[39m create_cnn_model()\n\u001b[1;32m     44\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39msummary()\n\u001b[0;32m---> 46\u001b[0m f1_m \u001b[38;5;241m=\u001b[39m \u001b[43mtfa\u001b[49m\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mF1Score(\n\u001b[1;32m     47\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m,\n\u001b[1;32m     48\u001b[0m     average \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmacro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     49\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     50\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_macro\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     51\u001b[0m     )\n\u001b[1;32m     53\u001b[0m f1_w \u001b[38;5;241m=\u001b[39m tfa\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mF1Score(\n\u001b[1;32m     54\u001b[0m     num_classes \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m11\u001b[39m,\n\u001b[1;32m     55\u001b[0m     average \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mweighted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     56\u001b[0m     threshold \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m     57\u001b[0m     name \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mf1_weighted\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     58\u001b[0m     )\n\u001b[1;32m     60\u001b[0m cnn_model\u001b[38;5;241m.\u001b[39mcompile(\n\u001b[1;32m     61\u001b[0m     optimizer\u001b[38;5;241m=\u001b[39mtf\u001b[38;5;241m.\u001b[39mkeras\u001b[38;5;241m.\u001b[39moptimizers\u001b[38;5;241m.\u001b[39mAdam(), \u001b[38;5;66;03m#learning rate defined by scheduler\u001b[39;00m\n\u001b[1;32m     62\u001b[0m     loss\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcategorical_crossentropy\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     63\u001b[0m     metrics \u001b[38;5;241m=\u001b[39m [\u001b[38;5;124m'\u001b[39m\u001b[38;5;124maccuracy\u001b[39m\u001b[38;5;124m'\u001b[39m, keras\u001b[38;5;241m.\u001b[39mmetrics\u001b[38;5;241m.\u001b[39mPrecision(), f1_m, f1_w])\n",
      "\u001b[0;31mNameError\u001b[0m: name 'tfa' is not defined"
     ]
    }
   ],
   "source": [
    "# From previous experiments, adding layers and using ResNet-18 yielded best results \n",
    "\n",
    "# x: Satellite Images, 'Stop_Signs', 'Paving_historical', 'Bus_stop', 'Collisions_Historical', Road type (one hot), Zipcode (ont hot)\n",
    "# y: Future collision bin\n",
    "\n",
    "# ACCORDING TO THE PAPER, THEY DOWNSIZED THE IMAGES FIRST INTO SMALLER MATRICES \n",
    "# THEN ADDED THE STREET CHARACTERISTICS TO THE SMALLER MATRIX\n",
    "# REFERENCE THE DIAGRAM ON THE PAPER\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    # INPUT LAYERS\n",
    "    input1 = tf.keras.layers.Input(shape=(148, 188, 4), name='Input_Images')\n",
    "    input2 = tf.keras.layers.Input(shape=(50,50,39), name='Input_Street') \n",
    "    \n",
    "    #CNN FOR IMAGE PROCESSING\n",
    "    cnn = tf.keras.layers.Conv2D(6, (3,3), activation=\"relu\")(input1) # layers = street data dimension, changed dimensions to be according to paper\n",
    "    pooling = tf.keras.layers.MaxPooling2D((99, 139), strides=1)(cnn) #downsizing, per paper\n",
    "    images = tf.keras.models.Model(inputs=input1, outputs=pooling)\n",
    "    \n",
    "    #ADDING STREET DATA\n",
    "    cnn2 = tf.keras.layers.Conv2D(2, (3,3), activation=\"relu\")(input2)\n",
    "    street = tf.keras.models.Model(inputs=input2, outputs=cnn2)\n",
    "    \n",
    "    combined = tf.keras.layers.Concatenate(axis = 3)([images.output, street.output])\n",
    "    #combined = tf.keras.layers.Add()([images.output, street.output])\n",
    "    \n",
    "    # PAPER USES RESNET-18 FOR THE REST OF THE MODEL WITH THE COMBINED DATA\n",
    "    #resnet = resnet18(combined)\n",
    "    \n",
    "    cnn = tf.keras.layers.Conv2D(39, (2,2), activation=\"relu\")(combined) # layers = street data dimension\n",
    "    \n",
    "    x = layers.GlobalAveragePooling2D(name='avgpool')(cnn)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=11, activation='softmax', name='output')(x) # units = number of classes\n",
    "    \n",
    "    #instantiation layer \n",
    "    cnn_model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "f1_m = tfa.metrics.F1Score(\n",
    "    num_classes = 11,\n",
    "    average = 'macro',\n",
    "    threshold = None,\n",
    "    name = 'f1_macro',\n",
    "    )\n",
    "\n",
    "f1_w = tfa.metrics.F1Score(\n",
    "    num_classes = 11,\n",
    "    average = 'weighted',\n",
    "    threshold = None,\n",
    "    name = 'f1_weighted',\n",
    "    )\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(), #learning rate defined by scheduler\n",
    "    loss='categorical_crossentropy',\n",
    "    metrics = ['accuracy', keras.metrics.Precision(), f1_m, f1_w])\n",
    "\n",
    "y_t = tf.keras.utils.to_categorical(y_train, num_classes=11)\n",
    "\n",
    "y_v = tf.keras.utils.to_categorical(y_val, num_classes=11)\n",
    "\n",
    "# define the checkpoint\n",
    "filepath = \"model.9\"\n",
    "checkpoint = ModelCheckpoint(filepath, monitor='val_f1_weighted', verbose=1, save_best_only=True, mode='max')\n",
    "callbacks_check = checkpoint\n",
    "\n",
    "def scheduler(epoch, lr):\n",
    "    if epoch < 4:\n",
    "        return 0.0001\n",
    "    elif epoch < 6:\n",
    "        return 0.00001\n",
    "    else:\n",
    "        return 0.000001\n",
    "\n",
    "callback = tf.keras.callbacks.LearningRateScheduler(scheduler)\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    [images_mini, street_mini_2],\n",
    "    y_t,\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "     verbose=1,\n",
    "    validation_data=([images_mini_v, street_mini_2_v], y_v),\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    #validation_split = 0.2,\n",
    "    class_weight = class_weights,\n",
    "    callbacks=[callback, callbacks_check],\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52d91844-64dc-4348-b024-783281f26bee",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# saving last epoch model just in case \n",
    "cnn_model.save('model.9.last.epoch')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cccdeff-f234-4fe1-b16b-c5db3c38830e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be7c6bdb-832f-43fa-bcf9-d5908aafdb77",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e805e4cd-dcb7-4568-845a-2e4e62d8a920",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1_macro(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf08e9fe-11ef-4aea-9cea-d4cd5bc26312",
   "metadata": {},
   "outputs": [],
   "source": [
    "plot_f1_weighted(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcbe81ee-3020-46ca-a1eb-93acd0da22c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred2 = cnn_model.predict([images_mini_v, street_mini_2_v])\n",
    "y_pred2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37b563fc-8984-4b33-9ede-bee946fa2f38",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "pred_values2 = []\n",
    "for i in y_pred2: \n",
    "    input_list = i\n",
    "    max_value = max(input_list)\n",
    "    index = [index for index, item in enumerate(input_list) if item == max_value]\n",
    "    pred_values2.append(index[0])\n",
    "pred_values2[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d862034d-b68b-4ebc-92dd-f0cbf5f39c00",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_true = list(y_val)\n",
    "y_true[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "62a56d10-ec48-4cb6-9e9a-10490eb6ac37",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "cm = confusion_matrix(y_true, pred_values2)\n",
    "ind = max(len(np.unique(y_true)), len(np.unique(pred_values2)))\n",
    "df_cm = pd.DataFrame(cm, index = [i for i in range(ind)],\n",
    "                  columns = [i for i in range(ind)])\n",
    "df_cm\n",
    "plt.figure(figsize = (10,7))\n",
    "plt.title('Confusion Matrix')\n",
    "plt.ylabel('Actual Values')\n",
    "plt.xlabel('Predicted Values')\n",
    "sn.heatmap(df_cm, annot=True, fmt='.3g',cmap=\"OrRd\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
