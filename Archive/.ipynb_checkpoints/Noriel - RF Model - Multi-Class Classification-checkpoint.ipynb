{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a0eabcba-5ca5-4fa8-bde3-8224131c3a6a",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356025b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 21:03:37.310159: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-18 21:03:38.310942: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-18 21:03:38.310967: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-18 21:03:40.714541: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-18 21:03:40.714647: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-18 21:03:40.714659: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import math\n",
    "\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "import tifffile\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "\n",
    "import os\n",
    "\n",
    "import scipy"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b166dc2-360e-4118-9f65-0996bbc163a1",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c1eeab-df74-4a28-8b0f-7d937d42028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "   # plt.plot(history.history['accuracy'], label='accuracy')\n",
    "   # plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['loss']),\n",
    "                max(history.history['val_loss'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d27acab-1145-4120-9a1c-c7da15f505d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "   # plt.plot(history.history['loss'], label='loss')\n",
    "   # plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['accuracy']),\n",
    "                max(history.history['val_accuracy'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "dd28b853",
   "metadata": {},
   "outputs": [],
   "source": [
    "# FROM https://github.com/jimmyyhwu/resnet18-tf2/blob/master/resnet.py\n",
    "\n",
    "kaiming_normal = keras.initializers.VarianceScaling(scale=2.0, mode='fan_out', distribution='untruncated_normal')\n",
    "\n",
    "def conv3x3(x, out_planes, stride=1, name=None):\n",
    "    x = layers.ZeroPadding2D(padding=1, name=f'{name}_pad')(x)\n",
    "    return layers.Conv2D(filters=out_planes, kernel_size=3, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=name)(x)\n",
    "\n",
    "def basic_block(x, planes, stride=1, downsample=None, name=None):\n",
    "    identity = x\n",
    "\n",
    "    out = conv3x3(x, planes, stride=stride, name=f'{name}.conv1')\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn1')(out)\n",
    "    out = layers.ReLU(name=f'{name}.relu1')(out)\n",
    "\n",
    "    out = conv3x3(out, planes, name=f'{name}.conv2')\n",
    "    out = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.bn2')(out)\n",
    "\n",
    "    if downsample is not None:\n",
    "        for layer in downsample:\n",
    "            identity = layer(identity)\n",
    "\n",
    "    out = layers.Add(name=f'{name}.add')([identity, out])\n",
    "    out = layers.ReLU(name=f'{name}.relu2')(out)\n",
    "\n",
    "    return out\n",
    "\n",
    "def make_layer(x, planes, blocks, stride=1, name=None):\n",
    "    downsample = None\n",
    "    inplanes = x.shape[3]\n",
    "    if stride != 1 or inplanes != planes:\n",
    "        downsample = [\n",
    "            layers.Conv2D(filters=planes, kernel_size=1, strides=stride, use_bias=False, kernel_initializer=kaiming_normal, name=f'{name}.0.downsample.0'),\n",
    "            layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name=f'{name}.0.downsample.1'),\n",
    "        ]\n",
    "\n",
    "    x = basic_block(x, planes, stride, downsample, name=f'{name}.0')\n",
    "    for i in range(1, blocks):\n",
    "        x = basic_block(x, planes, name=f'{name}.{i}')\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet(x, blocks_per_layer, num_classes=1000):\n",
    "    x = layers.ZeroPadding2D(padding=3, name='conv1_pad')(x)\n",
    "    x = layers.Conv2D(filters=64, kernel_size=7, strides=2, use_bias=False, kernel_initializer=kaiming_normal, name='conv1')(x)\n",
    "    x = layers.BatchNormalization(momentum=0.9, epsilon=1e-5, name='bn1')(x)\n",
    "    x = layers.ReLU(name='relu1')(x)\n",
    "    x = layers.ZeroPadding2D(padding=1, name='maxpool_pad')(x)\n",
    "    x = layers.MaxPool2D(pool_size=3, strides=2, name='maxpool')(x)\n",
    "\n",
    "    x = make_layer(x, 64, blocks_per_layer[0], name='layer1')\n",
    "    x = make_layer(x, 128, blocks_per_layer[1], stride=2, name='layer2')\n",
    "    x = make_layer(x, 256, blocks_per_layer[2], stride=2, name='layer3')\n",
    "    x = make_layer(x, 512, blocks_per_layer[3], stride=2, name='layer4')\n",
    "\n",
    "    x = layers.GlobalAveragePooling2D(name='avgpool')(x)\n",
    "    initializer = keras.initializers.RandomUniform(-1.0 / math.sqrt(512), 1.0 / math.sqrt(512))\n",
    "    x = layers.Dense(units=num_classes, kernel_initializer=initializer, bias_initializer=initializer, name='fc')(x)\n",
    "\n",
    "    return x\n",
    "\n",
    "def resnet18(x, **kwargs):\n",
    "    return resnet(x, [2, 2, 2, 2], **kwargs)\n",
    "\n",
    "def resnet34(x, **kwargs):\n",
    "    return resnet(x, [3, 4, 6, 3], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a5b49-a808-4d41-ad4b-ae9469b994ea",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f3693092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile_ID</th>\n",
       "      <th>Long2</th>\n",
       "      <th>Lat2</th>\n",
       "      <th>Long1</th>\n",
       "      <th>Lat1</th>\n",
       "      <th>Mid_lat</th>\n",
       "      <th>Mid_long</th>\n",
       "      <th>Stop_Signs</th>\n",
       "      <th>Paving_historical</th>\n",
       "      <th>Paving_future</th>\n",
       "      <th>Bus_stop</th>\n",
       "      <th>Collisions_Future</th>\n",
       "      <th>Collisions_Historical</th>\n",
       "      <th>RTTYP</th>\n",
       "      <th>bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>-122.514446</td>\n",
       "      <td>37.779636</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>37.779184</td>\n",
       "      <td>-122.513876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>-122.514446</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>37.778280</td>\n",
       "      <td>-122.513876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.779636</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>37.779184</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>37.778280</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.776925</td>\n",
       "      <td>37.777377</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>M</td>\n",
       "      <td>A</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tile_ID       Long2       Lat2       Long1       Lat1    Mid_lat  \\\n",
       "0       36 -122.514446  37.779636 -122.513306  37.778732  37.779184   \n",
       "1       37 -122.514446  37.778732 -122.513306  37.777829  37.778280   \n",
       "2      151 -122.513306  37.779636 -122.512166  37.778732  37.779184   \n",
       "3      152 -122.513306  37.778732 -122.512166  37.777829  37.778280   \n",
       "4      153 -122.513306  37.777829 -122.512166  37.776925  37.777377   \n",
       "\n",
       "     Mid_long  Stop_Signs  Paving_historical  Paving_future  Bus_stop  \\\n",
       "0 -122.513876         0.0                0.0            0.0       0.0   \n",
       "1 -122.513876         0.0                0.0            0.0       0.0   \n",
       "2 -122.512736         0.0                0.0            0.0       0.0   \n",
       "3 -122.512736         0.0                0.0            0.0       0.0   \n",
       "4 -122.512736         0.0                0.0            0.0       0.0   \n",
       "\n",
       "   Collisions_Future  Collisions_Historical RTTYP bin  \n",
       "0                0.0                    0.0     M   A  \n",
       "1                0.0                    0.0     M   A  \n",
       "2                0.0                    0.0     M   A  \n",
       "3                0.0                    0.0     M   A  \n",
       "4                0.0                    0.0     M   A  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles = pd.read_csv(r'./Data/Tiles_binned.csv')\n",
    "tiles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "460f0a51-0215-441f-ba8b-b128c308a998",
   "metadata": {},
   "source": [
    "https://stackoverflow.com/questions/66227003/reverse-geocoding-getting-postal-code-with-geopy-nominatim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "id": "28a2f0db-5324-4813-939d-1d1179dc68e5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#!pip3 install geopy\n",
    "import geopy\n",
    "import pandas as pd\n",
    "\n",
    "def get_zipcode(df, geolocator, lat_field, lon_field, attempt=1, max_attempts=100):\n",
    "    try:\n",
    "        location = geolocator.reverse((df[lat_field], df[lon_field]), timeout=None)\n",
    "        return location.raw['address']['postcode']\n",
    "    except KeyError:\n",
    "        pass\n",
    "    except GeocoderTimedOut:\n",
    "        if attempt <= max_attempts:\n",
    "            return get_zipcode (df, attempt=attempt+1)\n",
    "        raise\n",
    "\n",
    "geolocator = geopy.Nominatim(user_agent='1234')\n",
    "#geolocator = geopy.Nominatim(user_agent='my-application')\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'Lat': [29.39291, 29.39923, 29.40147, 29.38752, 29.39291, 29.39537, 29.39343, 29.39291, 29.39556],\n",
    "    'Lon': [-98.50925, -98.51256, -98.51123, -98.52372, -98.50925, -98.50402, -98.49707, -98.50925, -98.53148]\n",
    "})\n",
    "zipcodes = df.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='Lat', lon_field='Lon')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "e9d87604-d12c-4d93-b51a-9826e685a36d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    78204\n",
       "1    78204\n",
       "2    78204\n",
       "3    78225\n",
       "4    78204\n",
       "5    78204\n",
       "6    78204\n",
       "7    78204\n",
       "8    78225\n",
       "dtype: object"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zipcodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6add420-44cd-4a32-9bcb-6a1f974c8cec",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zipcodes1 = tiles.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='Lat1', lon_field='Long1')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "939c20e8-e1c1-44d5-93e6-8e8f58280f24",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zipcodes2 = tiles.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='Lat2', lon_field='Long2')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07133d32-9ab7-4145-8b10-a320db87cd4f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "zipcodes3 = tiles.apply(get_zipcode, axis=1, geolocator=geolocator, lat_field='Mid_lat', lon_field='Mid_long')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "7e18162b-88f8-4b5d-b4c1-e7bd9c3ca7f7",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Tile_ID', 'Long2', 'Lat2', 'Long1', 'Lat1', 'Mid_lat', 'Mid_long',\n",
       "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
       "       'Collisions_Future', 'Collisions_Historical', 'RTTYP', 'bin'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6b82c",
   "metadata": {},
   "source": [
    "Split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "0d50fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tiles[['Tile_ID', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical', 'RTTYP']], \n",
    "                                   tiles['bin'],\n",
    "                                   random_state=104, \n",
    "                                   test_size=0.20, \n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf220283",
   "metadata": {},
   "source": [
    "Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "9298e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part1(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    \n",
    "    for id in x_train['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]\n",
    "            \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "    \n",
    "    return images_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "4d8f77ae",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8376, 148, 188, 4)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini = preprocess_data_part1(IMAGE_PATH)\n",
    "np.shape(images_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "23790420-0c98-426f-8580-23f7d96e9040",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part2(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    \n",
    "    for id in x_test['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]\n",
    "            \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "    \n",
    "    return images_mini"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cc8e3f65-2368-4092-9b09-ecda4e0a35d6",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095, 148, 188, 4)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "images_mini_t = preprocess_data_part2(IMAGE_PATH)\n",
    "np.shape(images_mini_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57135842",
   "metadata": {},
   "source": [
    "Street Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "8dce0686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8376, 1, 1, 9)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street = np.asarray(x_train[['Tile_ID', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical']]).astype('float32')\n",
    "street_mini = []\n",
    "for row in range(len(street)):\n",
    "    street_mini.append([[street[row]]])\n",
    "street_mini = np.stack(street_mini)\n",
    "np.shape(street_mini)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "c28b0428-174b-4e10-b151-1354cf44bf79",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095, 1, 1, 9)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "street_t = np.asarray(x_test[['Tile_ID', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical']]).astype('float32')\n",
    "street_mini_t = []\n",
    "for row in range(len(street_t)):\n",
    "    street_mini_t.append([[street_t[row]]])\n",
    "street_mini_t = np.stack(street_mini_t)\n",
    "np.shape(street_mini_t)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae0d0a-47df-45ce-8bd4-9c31ed9d0cc3",
   "metadata": {},
   "source": [
    "**Noriel's Models**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "d2677cbd-a7b1-4e7b-91d7-532bac37cee4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "from numba import cuda \n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_decision_forests as tfdf\n",
    "import tifffile\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "\n",
    "\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelBinarizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b6335f7f-ee80-4aba-a63e-939320dfb117",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#[images_mini, street_mini]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "a13e660c-d9ed-4991-ba6a-c0cece399b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8419     C\n",
       "10129    B\n",
       "7641     A\n",
       "5215     A\n",
       "7784     A\n",
       "        ..\n",
       "6310     B\n",
       "8846     B\n",
       "729      A\n",
       "5825     A\n",
       "8261     B\n",
       "Name: bin, Length: 8376, dtype: object"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7217a2b4-a4fb-4407-8f08-cf72e15cdbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "2e009aa6-17b0-4b25-a570-aba1ce6100ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, ..., 0, 0, 1])"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "b9f4f0f7-a7dc-43f6-94db-d59f5b204b2f",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Use /tmp/tmp4rlgcie4 as temporary training directory\n",
      "Reading training dataset...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-18 21:05:07.722073: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-18 21:05:07.722932: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-18 21:05:07.722965: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-13-91): /proc/driver/nvidia/version does not exist\n",
      "2023-03-18 21:05:07.724884: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/capstone/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/ubuntu/capstone/lib/python3.10/site-packages/tensorflow/python/autograph/pyct/static_analysis/liveness.py:83: Analyzer.lamba_check (from tensorflow.python.autograph.pyct.static_analysis.liveness) is deprecated and will be removed after 2023-09-23.\n",
      "Instructions for updating:\n",
      "Lambda fuctions will be no more assumed to be used in the statement where they are used, or at least in the same block. https://github.com/tensorflow/tensorflow/issues/56089\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training dataset read in 0:00:04.647743. Found 8376 examples.\n",
      "Training model...\n",
      "Model trained in 0:00:00.224316\n",
      "Compiling model...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO 2023-03-18T21:05:12.671955615+00:00 kernel.cc:1214] Loading model from path /tmp/tmp4rlgcie4/model/ with prefix 25e24cb4d80243b0\n",
      "[INFO 2023-03-18T21:05:12.701286473+00:00 decision_forest.cc:661] Model loaded with 100 root(s), 9364 node(s), and 9 input feature(s).\n",
      "[INFO 2023-03-18T21:05:12.701325623+00:00 abstract_model.cc:1311] Engine \"RandomForestGeneric\" built\n",
      "[INFO 2023-03-18T21:05:12.701353649+00:00 kernel.cc:1046] Use fast generic engine\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f0778a472e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f0778a472e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING: AutoGraph could not transform <function simple_ml_inference_op_with_handle at 0x7f0778a472e0> and will run it as-is.\n",
      "Please report this to the TensorFlow team. When filing the bug, set the verbosity to 10 (on Linux, `export AUTOGRAPH_VERBOSITY=10`) and attach the full output.\n",
      "Cause: could not get source code\n",
      "To silence this warning, decorate the function with @tf.autograph.experimental.do_not_convert\n",
      "Model compiled.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<keras.callbacks.History at 0x7f0777fd18a0>"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rf_model = tfdf.keras.RandomForestModel(categorical_algorithm = 'CART', num_trees=100)\n",
    "\n",
    "rf_model.fit(np.asarray(x_train[['Tile_ID', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical']]).astype('float32'),\n",
    "    y_train,\n",
    "    #validation_data=[x_test[['Collisions_Historical', 'Mid_lat','Mid_long', 'Stop_Signs', 'Paving_historical', 'Bus_stop']], y_test],     \n",
    "    epochs=1,\n",
    "    verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "14110520-7657-467c-83af-ca78d8e431de",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"random_forest_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      "=================================================================\n",
      "Total params: 1\n",
      "Trainable params: 0\n",
      "Non-trainable params: 1\n",
      "_________________________________________________________________\n",
      "Type: \"RANDOM_FOREST\"\n",
      "Task: CLASSIFICATION\n",
      "Label: \"__LABEL\"\n",
      "\n",
      "Input Features (9):\n",
      "\tdata:0.0\n",
      "\tdata:0.1\n",
      "\tdata:0.2\n",
      "\tdata:0.3\n",
      "\tdata:0.4\n",
      "\tdata:0.5\n",
      "\tdata:0.6\n",
      "\tdata:0.7\n",
      "\tdata:0.8\n",
      "\n",
      "No weights\n",
      "\n",
      "Variable Importance: INV_MEAN_MIN_DEPTH:\n",
      "    1. \"data:0.7\"  0.387220 ################\n",
      "    2. \"data:0.8\"  0.330633 ############\n",
      "    3. \"data:0.0\"  0.181789 ###\n",
      "    4. \"data:0.3\"  0.176020 ###\n",
      "    5. \"data:0.6\"  0.162432 ##\n",
      "    6. \"data:0.1\"  0.152311 #\n",
      "    7. \"data:0.2\"  0.145518 #\n",
      "    8. \"data:0.4\"  0.126452 \n",
      "    9. \"data:0.5\"  0.125614 \n",
      "\n",
      "Variable Importance: NUM_AS_ROOT:\n",
      "    1. \"data:0.7\" 41.000000 ################\n",
      "    2. \"data:0.8\" 33.000000 ############\n",
      "    3. \"data:0.6\" 12.000000 ##\n",
      "    4. \"data:0.0\"  8.000000 \n",
      "    5. \"data:0.3\"  6.000000 \n",
      "\n",
      "Variable Importance: NUM_NODES:\n",
      "    1. \"data:0.7\" 2210.000000 ################\n",
      "    2. \"data:0.0\" 561.000000 ###\n",
      "    3. \"data:0.1\" 530.000000 ###\n",
      "    4. \"data:0.8\" 495.000000 ###\n",
      "    5. \"data:0.2\" 347.000000 ##\n",
      "    6. \"data:0.6\" 229.000000 #\n",
      "    7. \"data:0.3\" 199.000000 #\n",
      "    8. \"data:0.4\" 41.000000 \n",
      "    9. \"data:0.5\" 20.000000 \n",
      "\n",
      "Variable Importance: SUM_SCORE:\n",
      "    1. \"data:0.7\" 500410.157825 ################\n",
      "    2. \"data:0.8\" 67914.125610 ##\n",
      "    3. \"data:0.3\" 7910.496342 \n",
      "    4. \"data:0.0\" 5946.646595 \n",
      "    5. \"data:0.6\" 5821.139692 \n",
      "    6. \"data:0.1\" 3595.844945 \n",
      "    7. \"data:0.2\" 2253.580217 \n",
      "    8. \"data:0.4\" 319.633352 \n",
      "    9. \"data:0.5\" 106.470499 \n",
      "\n",
      "\n",
      "\n",
      "Winner takes all: true\n",
      "Out-of-bag evaluation: accuracy:0.997373 logloss:0.0334646\n",
      "Number of trees: 100\n",
      "Total number of nodes: 9364\n",
      "\n",
      "Number of nodes by tree:\n",
      "Count: 100 Average: 93.64 StdDev: 48.6275\n",
      "Min: 17 Max: 233 Ignored: 0\n",
      "----------------------------------------------\n",
      "[  17,  27)  5   5.00%   5.00% #####\n",
      "[  27,  38)  8   8.00%  13.00% ########\n",
      "[  38,  49)  7   7.00%  20.00% #######\n",
      "[  49,  60) 10  10.00%  30.00% ##########\n",
      "[  60,  71)  7   7.00%  37.00% #######\n",
      "[  71,  82)  4   4.00%  41.00% ####\n",
      "[  82,  92) 10  10.00%  51.00% ##########\n",
      "[  92, 103)  9   9.00%  60.00% #########\n",
      "[ 103, 114)  7   7.00%  67.00% #######\n",
      "[ 114, 125)  8   8.00%  75.00% ########\n",
      "[ 125, 136)  7   7.00%  82.00% #######\n",
      "[ 136, 147)  4   4.00%  86.00% ####\n",
      "[ 147, 158)  5   5.00%  91.00% #####\n",
      "[ 158, 168)  1   1.00%  92.00% #\n",
      "[ 168, 179)  0   0.00%  92.00%\n",
      "[ 179, 190)  3   3.00%  95.00% ###\n",
      "[ 190, 201)  2   2.00%  97.00% ##\n",
      "[ 201, 212)  1   1.00%  98.00% #\n",
      "[ 212, 223)  1   1.00%  99.00% #\n",
      "[ 223, 233]  1   1.00% 100.00% #\n",
      "\n",
      "Depth by leafs:\n",
      "Count: 4732 Average: 7.35609 StdDev: 2.12237\n",
      "Min: 1 Max: 14 Ignored: 0\n",
      "----------------------------------------------\n",
      "[  1,  2)  41   0.87%   0.87%\n",
      "[  2,  3)  75   1.58%   2.45% #\n",
      "[  3,  4) 105   2.22%   4.67% #\n",
      "[  4,  5) 192   4.06%   8.73% ##\n",
      "[  5,  6) 388   8.20%  16.93% ####\n",
      "[  6,  7) 688  14.54%  31.47% #######\n",
      "[  7,  8) 915  19.34%  50.80% ##########\n",
      "[  8,  9) 949  20.05%  70.86% ##########\n",
      "[  9, 10) 729  15.41%  86.26% ########\n",
      "[ 10, 11) 368   7.78%  94.04% ####\n",
      "[ 11, 12) 189   3.99%  98.03% ##\n",
      "[ 12, 13)  67   1.42%  99.45% #\n",
      "[ 13, 14)  18   0.38%  99.83%\n",
      "[ 14, 14]   8   0.17% 100.00%\n",
      "\n",
      "Number of training obs by leaf:\n",
      "Count: 4732 Average: 177.008 StdDev: 840.95\n",
      "Min: 5 Max: 6706 Ignored: 0\n",
      "----------------------------------------------\n",
      "[    5,  340) 4431  93.64%  93.64% ##########\n",
      "[  340,  675)  116   2.45%  96.09%\n",
      "[  675, 1010)   33   0.70%  96.79%\n",
      "[ 1010, 1345)   29   0.61%  97.40%\n",
      "[ 1345, 1680)    8   0.17%  97.57%\n",
      "[ 1680, 2015)    9   0.19%  97.76%\n",
      "[ 2015, 2350)    4   0.08%  97.84%\n",
      "[ 2350, 2685)    2   0.04%  97.89%\n",
      "[ 2685, 3020)    3   0.06%  97.95%\n",
      "[ 3020, 3356)    4   0.08%  98.03%\n",
      "[ 3356, 3691)    4   0.08%  98.12%\n",
      "[ 3691, 4026)    1   0.02%  98.14%\n",
      "[ 4026, 4361)    3   0.06%  98.20%\n",
      "[ 4361, 4696)   10   0.21%  98.42%\n",
      "[ 4696, 5031)    1   0.02%  98.44%\n",
      "[ 5031, 5366)    7   0.15%  98.58%\n",
      "[ 5366, 5701)    2   0.04%  98.63%\n",
      "[ 5701, 6036)   24   0.51%  99.13%\n",
      "[ 6036, 6371)    0   0.00%  99.13%\n",
      "[ 6371, 6706]   41   0.87% 100.00%\n",
      "\n",
      "Attribute in nodes:\n",
      "\t2210 : data:0.7 [NUMERICAL]\n",
      "\t561 : data:0.0 [NUMERICAL]\n",
      "\t530 : data:0.1 [NUMERICAL]\n",
      "\t495 : data:0.8 [NUMERICAL]\n",
      "\t347 : data:0.2 [NUMERICAL]\n",
      "\t229 : data:0.6 [NUMERICAL]\n",
      "\t199 : data:0.3 [NUMERICAL]\n",
      "\t41 : data:0.4 [NUMERICAL]\n",
      "\t20 : data:0.5 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 0:\n",
      "\t41 : data:0.7 [NUMERICAL]\n",
      "\t33 : data:0.8 [NUMERICAL]\n",
      "\t12 : data:0.6 [NUMERICAL]\n",
      "\t8 : data:0.0 [NUMERICAL]\n",
      "\t6 : data:0.3 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 1:\n",
      "\t116 : data:0.7 [NUMERICAL]\n",
      "\t69 : data:0.8 [NUMERICAL]\n",
      "\t30 : data:0.3 [NUMERICAL]\n",
      "\t22 : data:0.6 [NUMERICAL]\n",
      "\t15 : data:0.0 [NUMERICAL]\n",
      "\t3 : data:0.2 [NUMERICAL]\n",
      "\t3 : data:0.1 [NUMERICAL]\n",
      "\t1 : data:0.4 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 2:\n",
      "\t238 : data:0.7 [NUMERICAL]\n",
      "\t119 : data:0.8 [NUMERICAL]\n",
      "\t53 : data:0.3 [NUMERICAL]\n",
      "\t35 : data:0.6 [NUMERICAL]\n",
      "\t27 : data:0.0 [NUMERICAL]\n",
      "\t13 : data:0.2 [NUMERICAL]\n",
      "\t13 : data:0.1 [NUMERICAL]\n",
      "\t3 : data:0.4 [NUMERICAL]\n",
      "\t1 : data:0.5 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 3:\n",
      "\t421 : data:0.7 [NUMERICAL]\n",
      "\t181 : data:0.8 [NUMERICAL]\n",
      "\t74 : data:0.3 [NUMERICAL]\n",
      "\t68 : data:0.0 [NUMERICAL]\n",
      "\t57 : data:0.6 [NUMERICAL]\n",
      "\t42 : data:0.1 [NUMERICAL]\n",
      "\t29 : data:0.2 [NUMERICAL]\n",
      "\t9 : data:0.4 [NUMERICAL]\n",
      "\t2 : data:0.5 [NUMERICAL]\n",
      "\n",
      "Attribute in nodes with depth <= 5:\n",
      "\t1079 : data:0.7 [NUMERICAL]\n",
      "\t310 : data:0.8 [NUMERICAL]\n",
      "\t231 : data:0.0 [NUMERICAL]\n",
      "\t196 : data:0.1 [NUMERICAL]\n",
      "\t120 : data:0.3 [NUMERICAL]\n",
      "\t120 : data:0.2 [NUMERICAL]\n",
      "\t119 : data:0.6 [NUMERICAL]\n",
      "\t20 : data:0.4 [NUMERICAL]\n",
      "\t10 : data:0.5 [NUMERICAL]\n",
      "\n",
      "Condition type in nodes:\n",
      "\t4632 : HigherCondition\n",
      "Condition type in nodes with depth <= 0:\n",
      "\t100 : HigherCondition\n",
      "Condition type in nodes with depth <= 1:\n",
      "\t259 : HigherCondition\n",
      "Condition type in nodes with depth <= 2:\n",
      "\t502 : HigherCondition\n",
      "Condition type in nodes with depth <= 3:\n",
      "\t883 : HigherCondition\n",
      "Condition type in nodes with depth <= 5:\n",
      "\t2205 : HigherCondition\n",
      "Node format: NOT_SET\n",
      "\n",
      "Training OOB:\n",
      "\ttrees: 1, Out-of-bag evaluation: accuracy:0.991632 logloss:0.301621\n",
      "\ttrees: 11, Out-of-bag evaluation: accuracy:0.99159 logloss:0.14012\n",
      "\ttrees: 21, Out-of-bag evaluation: accuracy:0.995105 logloss:0.0593785\n",
      "\ttrees: 31, Out-of-bag evaluation: accuracy:0.995821 logloss:0.0455395\n",
      "\ttrees: 41, Out-of-bag evaluation: accuracy:0.996538 logloss:0.0460632\n",
      "\ttrees: 51, Out-of-bag evaluation: accuracy:0.997135 logloss:0.0408028\n",
      "\ttrees: 61, Out-of-bag evaluation: accuracy:0.997254 logloss:0.0414496\n",
      "\ttrees: 71, Out-of-bag evaluation: accuracy:0.997373 logloss:0.0334899\n",
      "\ttrees: 81, Out-of-bag evaluation: accuracy:0.997493 logloss:0.0337005\n",
      "\ttrees: 91, Out-of-bag evaluation: accuracy:0.997373 logloss:0.0333357\n",
      "\ttrees: 100, Out-of-bag evaluation: accuracy:0.997373 logloss:0.0334646\n",
      "\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "print(rf_model.summary())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "5b594c40-eec6-4cff-afc6-c93870de0de5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Evaluation(num_examples=8376, accuracy=0.9973734479465138, loss=0.03346462404633825, rmse=None, ndcg=None, aucs=None, auuc=None, qini=None)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "##uncomment to show training log\n",
    "#rf_model_train.make_inspector().training_logs()\n",
    "rf_model.make_inspector().evaluation()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7078a13f-031f-4516-befd-3e8bbad12679",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "262/262 [==============================] - 0s 1ms/step\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.010000</td>\n",
       "      <td>0.959999</td>\n",
       "      <td>0.02</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8371</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8372</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8373</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8374</th>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8375</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.999999</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8376 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "            0         1         2     3     4    5    6    7    8    9    10\n",
       "0     0.000000  0.010000  0.959999  0.02  0.01  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "1     0.000000  0.999999  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "2     0.999999  0.000000  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "3     0.999999  0.000000  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "4     0.999999  0.000000  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "...        ...       ...       ...   ...   ...  ...  ...  ...  ...  ...  ...\n",
       "8371  0.000000  0.999999  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8372  0.000000  0.999999  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8373  0.999999  0.000000  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8374  0.999999  0.000000  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "8375  0.000000  0.999999  0.000000  0.00  0.00  0.0  0.0  0.0  0.0  0.0  0.0\n",
       "\n",
       "[8376 rows x 11 columns]"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.DataFrame(rf_model.predict(x_train[['Tile_ID', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical']]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "8af3edc9-2e32-4bc8-8c5b-5bf3085d8060",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "logs = rf_model.make_inspector().training_logs()\n",
    "\n",
    "plt.plot([log.evaluation.num_examples for log in logs], [log.evaluation.loss for log in logs], label=\"training data\")\n",
    "plt.xlabel(\"Number of trees\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1ec0dbb-2532-4c3a-b053-1f6f687be79c",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc8f8956",
   "metadata": {},
   "source": [
    "Helpful Links: <br>\n",
    "https://pyimagesearch.com/2019/02/04/keras-multiple-inputs-and-mixed-data/ <br>\n",
    "https://www.tensorflow.org/api_docs/python/tf/keras/applications/resnet50/ResNet50 <br>\n",
    "https://github.com/jimmyyhwu/resnet18-tf2/blob/master/resnet.py <BR>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a032bd29",
   "metadata": {},
   "source": [
    "**Model 2.1: CNN (layers added, ResNet-18)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b198024",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x: Satellite Images, 'Collisions_Historical', 'Mid_lat','Mid_long', 'Stop_Signs', 'Paving_historical', 'Bus_stop'\n",
    "# y: 'Collisions_Future'\n",
    "\n",
    "# ACCORDING TO THE PAPER, THEY DOWNSIZED THE IMAGES FIRST INTO SMALLER MATRICES \n",
    "# THEN ADDED THE STREET CHARACTERISTICS TO THE SMALLER MATRIX\n",
    "# REFERENCE THE DIAGRAM ON THE PAPER\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    # INPUT LAYERS\n",
    "    input1 = tf.keras.layers.Input(shape=(148, 188, 4), name='Input_Images')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,1,11), name='Input_Street')\n",
    "    \n",
    "    #CNN FOR IMAGE PROCESSING\n",
    "    cnn = tf.keras.layers.Conv2D(11, (4,4), activation=\"relu\")(input1)\n",
    "    pooling = tf.keras.layers.MaxPooling2D((4, 4), strides=2)(cnn)\n",
    "    images = tf.keras.models.Model(inputs=input1, outputs=pooling)\n",
    "    \n",
    "    #ADDING STREET DATA\n",
    "    #combined = tf.keras.layers.Concatenate(axis = 2)([images.output, input2])\n",
    "    combined = tf.keras.layers.Add()([images.output, input2])\n",
    "    \n",
    "    # PAPER USES RESNET-18 FOR THE REST OF THE MODEL WITH THE COMBINED DATA\n",
    "    resnet = resnet18(combined)\n",
    "    output = tf.keras.layers.Dense(units=12, activation='softmax', name='output')(resnet)\n",
    "    \n",
    "    #instantiation layer \n",
    "    cnn_model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "class_weight = {0: 100,\n",
    "                1: 1000,\n",
    "                2: 1000,\n",
    "                3: 1000,\n",
    "                4: 1000,\n",
    "                5: 1000,\n",
    "                6: 1000,\n",
    "                7: 1000,\n",
    "                8: 1000,\n",
    "                9: 1000,\n",
    "                10: 1000,\n",
    "                11: 1000,\n",
    "               }\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    [images_mini, street_mini],\n",
    "    np.stack(y_train),\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "     verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2,\n",
    "    class_weight = class_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "529937cd",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "efd6816d-4465-4a90-b2f4-6930be4d6efa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4b29fdc-932b-4504-813b-a692525348e1",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = cnn_model.predict([images_mini_t, street_mini_t])\n",
    "test_values = []\n",
    "for i in predicted_result: \n",
    "    input_list = i\n",
    "    max_value = max(input_list)\n",
    "    index = [index for index, item in enumerate(input_list) if item == max_value]\n",
    "    test_values.append(index[0])\n",
    "print('macro f1: ', f1_score(y_test, test_values, average = 'macro' ))\n",
    "print('f1 by class: ', f1_score(y_test, test_values, average = None ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc3fbae2-bbfa-416f-ba2c-7fae81b75d67",
   "metadata": {},
   "source": [
    "**Model 2.2: Second CNN (layers added, ResNet-34)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cdb0ab1-0afc-439b-823b-43932327e271",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x: Satellite Images, 'Collisions_Historical', 'Mid_lat','Mid_long', 'Stop_Signs', 'Paving_historical', 'Bus_stop'\n",
    "# y: 'Collisions_Future'\n",
    "\n",
    "# ACCORDING TO THE PAPER, THEY DOWNSIZED THE IMAGES FIRST INTO SMALLER MATRICES \n",
    "# THEN ADDED THE STREET CHARACTERISTICS TO THE SMALLER MATRIX\n",
    "# REFERENCE THE DIAGRAM ON THE PAPER\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    # INPUT LAYERS\n",
    "    input1 = tf.keras.layers.Input(shape=(148, 188, 4), name='Input_Images')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,1,11), name='Input_Street')\n",
    "    \n",
    "    #CNN FOR IMAGE PROCESSING\n",
    "    cnn = tf.keras.layers.Conv2D(11, (4,4), activation=\"relu\")(input1)\n",
    "    pooling = tf.keras.layers.MaxPooling2D((4, 4), strides=2)(cnn)\n",
    "    images = tf.keras.models.Model(inputs=input1, outputs=pooling)\n",
    "    \n",
    "    #ADDING STREET DATA\n",
    "    #combined = tf.keras.layers.Concatenate(axis = 2)([images.output, input2])\n",
    "    combined = tf.keras.layers.Add()([images.output, input2])\n",
    "    \n",
    "    # PAPER USES RESNET-18 FOR THE REST OF THE MODEL WITH THE COMBINED DATA\n",
    "    resnet = resnet34(combined)\n",
    "    output = tf.keras.layers.Dense(units=12, activation='softmax', name='output')(resnet)\n",
    "    \n",
    "    #instantiation layer \n",
    "    cnn_model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "class_weight = {0: 100,\n",
    "                1: 1000,\n",
    "                2: 1000,\n",
    "                3: 1000,\n",
    "                4: 1000,\n",
    "                5: 1000,\n",
    "                6: 1000,\n",
    "                7: 1000,\n",
    "                8: 1000,\n",
    "                9: 1000,\n",
    "                10: 1000,\n",
    "                11: 1000,\n",
    "               }\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    [images_mini, street_mini],\n",
    "    np.stack(y_train),\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "     verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2,\n",
    "    class_weight = class_weight, \n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "430dbe4a-ea51-4ab3-9b86-6906419144b8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cb61fd5-f83f-48fc-b445-ff2169f5c885",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb71b6e-667c-47fd-b36a-2ffe39bd22f1",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "predicted_result = cnn_model.predict([images_mini_t, street_mini_t])\n",
    "predicted_result[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8718b71a-4e14-4e1a-b17e-ffdb5e6f9d2a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "test_values = []\n",
    "for i in predicted_result: \n",
    "    input_list = i\n",
    "    max_value = max(input_list)\n",
    "    index = [index for index, item in enumerate(input_list) if item == max_value]\n",
    "    test_values.append(index[0])\n",
    "test_values[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2661b615-bbdb-43e4-9dee-23150f18c9cf",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "print('macro f1: ', f1_score(y_test, test_values, average = 'macro' ))\n",
    "print('f1 by class: ', f1_score(y_test, test_values, average = None ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b2117f5-3472-44d7-9b22-cba0914012d2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "9b54cf29-9972-4d60-9493-25bc651ada7f",
   "metadata": {},
   "source": [
    "**Model 2.3: CNN (layers concatenated, ResNet-18)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f3c69b-3b60-4e5e-a7fd-1005eecfec1a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "street_mini_2 = [] \n",
    "for k in range(len(street)):\n",
    "    for i in range(71): \n",
    "        for j in range(91):\n",
    "            street_mini_2.append(street[k])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "20b975a2-c6dc-4f07-a4ae-5c83d1bf9aaa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "street_mini_2 = np.reshape(street_mini_2, (len(street),71,91,11))\n",
    "np.shape(street_mini_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da44b747-8c12-427e-9d12-010dcfdd6c94",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2efe3d33-18de-4e73-9735-c1faaf102f25",
   "metadata": {},
   "outputs": [],
   "source": [
    "# x: Satellite Images, 'Collisions_Historical', 'Mid_lat','Mid_long', 'Stop_Signs', 'Paving_historical', 'Bus_stop'\n",
    "# y: 'Collisions_Future'\n",
    "\n",
    "# ACCORDING TO THE PAPER, THEY DOWNSIZED THE IMAGES FIRST INTO SMALLER MATRICES \n",
    "# THEN ADDED THE STREET CHARACTERISTICS TO THE SMALLER MATRIX\n",
    "# REFERENCE THE DIAGRAM ON THE PAPER\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    # INPUT LAYERS\n",
    "    input1 = tf.keras.layers.Input(shape=(148, 188, 4), name='Input_Images')\n",
    "    input2 = tf.keras.layers.Input(shape=(71,91,11), name='Input_Street')\n",
    "    \n",
    "    #CNN FOR IMAGE PROCESSING\n",
    "    cnn = tf.keras.layers.Conv2D(11, (4,4), activation=\"relu\")(input1)\n",
    "    pooling = tf.keras.layers.MaxPooling2D((4, 4), strides=2)(cnn)\n",
    "    images = tf.keras.models.Model(inputs=input1, outputs=pooling)\n",
    "    \n",
    "    #ADDING STREET DATA\n",
    "    combined = tf.keras.layers.Concatenate(axis = 3)([images.output, input2])\n",
    "    #combined = tf.keras.layers.Add()([images.output, input2])\n",
    "    \n",
    "    # PAPER USES RESNET-18 FOR THE REST OF THE MODEL WITH THE COMBINED DATA\n",
    "    resnet = resnet18(combined)\n",
    "    output = tf.keras.layers.Dense(units=12, activation='softmax', name='output')(resnet)\n",
    "    \n",
    "    #instantiation layer \n",
    "    cnn_model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    return cnn_model\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',\n",
    "    metrics = ['accuracy']\n",
    "    )\n",
    "\n",
    "class_weight = {0: 100,\n",
    "                1: 1000,\n",
    "                2: 1000,\n",
    "                3: 1000,\n",
    "                4: 1000,\n",
    "                5: 1000,\n",
    "                6: 1000,\n",
    "                7: 1000,\n",
    "                8: 1000,\n",
    "                9: 1000,\n",
    "                10: 1000,\n",
    "                11: 1000,\n",
    "               }\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    [images_mini, street_mini_2],\n",
    "    np.stack(y_train),\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "     verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2,\n",
    "    class_weight = class_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56133e1d-a889-4fb4-8850-a8afeb3e9ee5",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ecba962-271b-4404-80a8-acc856cb8696",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "debb65fa-626e-4355-b319-3f3bc190dd1c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "street_mini_2_t = [] \n",
    "for k in range(len(street_t)):\n",
    "    for i in range(71): \n",
    "        for j in range(91):\n",
    "            street_mini_2_t.append(street_t[k])\n",
    "street_mini_2_t = np.reshape(street_mini_2_t, (len(street_t),71,91,11))\n",
    "np.shape(street_mini_2_t)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0a2525c-833a-494c-92de-ae16b52dec7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = cnn_model.predict([images_mini_t, street_mini_2_t])\n",
    "test_values = []\n",
    "for i in predicted_result: \n",
    "    input_list = i\n",
    "    max_value = max(input_list)\n",
    "    index = [index for index, item in enumerate(input_list) if item == max_value]\n",
    "    test_values.append(index[0])\n",
    "print('macro f1: ', f1_score(y_test, test_values, average = 'macro' ))\n",
    "print('f1 by class: ', f1_score(y_test, test_values, average = None ))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "412ccfec-b018-48a1-9be9-45e67d98c005",
   "metadata": {},
   "source": [
    "**Model 2.4 CNN (layers added, RESNET 50 FROM TENSORFLOW)**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "752fd1a2-6a63-4219-b710-aa2aa2b8c5f8",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# x: Satellite Images, 'Collisions_Historical', 'Mid_lat','Mid_long', 'Stop_Signs', 'Paving_historical', 'Bus_stop'\n",
    "# y: 'Collisions_Future'\n",
    "\n",
    "# ACCORDING TO THE PAPER, THEY DOWNSIZED THE IMAGES FIRST INTO SMALLER MATRICES \n",
    "# THEN ADDED THE STREET CHARACTERISTICS TO THE SMALLER MATRIX\n",
    "# REFERENCE THE DIAGRAM ON THE PAPER\n",
    "\n",
    "def create_cnn_model():\n",
    "\n",
    "    # INPUT LAYERS\n",
    "    input1 = tf.keras.layers.Input(shape=(148, 188, 4), name='Input_Images')\n",
    "    input2 = tf.keras.layers.Input(shape=(1,1,11), name='Input_Street')\n",
    "    \n",
    "    #CNN FOR IMAGE PROCESSING\n",
    "    cnn = tf.keras.layers.Conv2D(11, (4,4), activation=\"relu\")(input1)\n",
    "    pooling = tf.keras.layers.MaxPooling2D((4, 4), strides=2)(cnn)\n",
    "    images = tf.keras.models.Model(inputs=input1, outputs=pooling)\n",
    "    \n",
    "    #ADDING STREET DATA\n",
    "    #combined = tf.keras.layers.Concatenate(axis = 2)([images.output, input2])\n",
    "    combined = tf.keras.layers.Add()([images.output, input2])\n",
    "    \n",
    "    # PAPER USES RESNET-18 FOR THE REST OF THE MODEL WITH THE COMBINED DATA\n",
    "    # RESNET50 FROM TENSORFLOW \n",
    "    resnet = tf.keras.applications.resnet50.ResNet50(\n",
    "    include_top=False,\n",
    "    weights=None,\n",
    "    input_tensor=combined,\n",
    "    input_shape=(71, 91, 11,),\n",
    "    pooling=None,\n",
    "    classes=12,\n",
    "    #**kwargs\n",
    "    )\n",
    "    \n",
    "    flatten = tf.keras.layers.Flatten()(resnet.output)\n",
    "    \n",
    "    output = tf.keras.layers.Dense(units=12, activation='softmax', name='output')(flatten)\n",
    "    \n",
    "    #instantiation layer \n",
    "    cnn_model = tf.keras.models.Model(inputs=[input1, input2], outputs=output)\n",
    "    \n",
    "    return cnn_model #cnn_model\n",
    "\n",
    "cnn_model = create_cnn_model()\n",
    "\n",
    "cnn_model.summary()\n",
    "\n",
    "cnn_model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss= ['sparse_categorical_crossentropy'],\n",
    "    metrics = ['accuracy'],\n",
    "    )\n",
    "\n",
    "class_weight = {0: 100,\n",
    "                1: 1000,\n",
    "                2: 1000,\n",
    "                3: 1000,\n",
    "                4: 1000,\n",
    "                5: 1000,\n",
    "                6: 1000,\n",
    "                7: 1000,\n",
    "                8: 1000,\n",
    "                9: 1000,\n",
    "                10: 1000,\n",
    "                11: 1000,\n",
    "               }\n",
    "\n",
    "history = cnn_model.fit(\n",
    "    [images_mini, street_mini],\n",
    "    np.stack(y_train),\n",
    "    epochs=10,\n",
    "    # Suppress logging.\n",
    "     verbose=1,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2,\n",
    "    class_weight = class_weight,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92081966-4f86-4889-a4bc-1d970d678459",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4f399215-cee3-458b-8b16-28857072dc6a",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_accuracy(history)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0491af29-b55b-4f1f-a061-c4887ddb538e",
   "metadata": {},
   "outputs": [],
   "source": [
    "predicted_result = cnn_model.predict([images_mini_t, street_mini_t])\n",
    "test_values = []\n",
    "for i in predicted_result: \n",
    "    input_list = i\n",
    "    max_value = max(input_list)\n",
    "    index = [index for index, item in enumerate(input_list) if item == max_value]\n",
    "    test_values.append(index[0])\n",
    "print('macro f1: ', f1_score(y_test, test_values, average = 'macro' ))\n",
    "print('f1 by class: ', f1_score(y_test, test_values, average = None ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4739af5b-a7e3-4888-964f-3c9f68c83fe3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
