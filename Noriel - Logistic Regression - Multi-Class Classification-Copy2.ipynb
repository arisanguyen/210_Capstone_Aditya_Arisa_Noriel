{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "ab39d93a-0a73-49d1-80fa-4fa51f6ec437",
   "metadata": {
    "tags": []
   },
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a0eabcba-5ca5-4fa8-bde3-8224131c3a6a",
   "metadata": {},
   "source": [
    "**Packages**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "356025b8",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 05:37:55.653004: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2023-03-22 05:37:55.779149: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcudart.so.11.0'; dlerror: libcudart.so.11.0: cannot open shared object file: No such file or directory\n",
      "2023-03-22 05:37:55.779170: I tensorflow/compiler/xla/stream_executor/cuda/cudart_stub.cc:29] Ignore above cudart dlerror if you do not have a GPU set up on your machine.\n",
      "2023-03-22 05:37:56.538040: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-22 05:37:56.538150: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory\n",
      "2023-03-22 05:37:56.538162: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    }
   ],
   "source": [
    "import math\n",
    "import matplotlib\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "import os\n",
    "\n",
    "import pandas as pd\n",
    "\n",
    "import scipy\n",
    "from sklearn import preprocessing\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.preprocessing import LabelBinarizer\n",
    "\n",
    "import tensorflow as tf\n",
    "import tensorflow_addons as tfa\n",
    "import tensorflow_decision_forests as tfdf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from tensorflow.keras.layers import concatenate\n",
    "from keras.layers import Flatten, Dense, Dropout\n",
    "import tifffile"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b166dc2-360e-4118-9f65-0996bbc163a1",
   "metadata": {},
   "source": [
    "**Helper Functions**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51c1eeab-df74-4a28-8b0f-7d937d42028c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_loss(history):\n",
    "    plt.plot(history.history['loss'], label='loss')\n",
    "    plt.plot(history.history['val_loss'], label='val_loss')\n",
    "   # plt.plot(history.history['accuracy'], label='accuracy')\n",
    "   # plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['loss']),\n",
    "                max(history.history['val_loss'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Loss')\n",
    "    plt.title('Loss')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "6d27acab-1145-4120-9a1c-c7da15f505d7",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def plot_accuracy(history):\n",
    "   # plt.plot(history.history['loss'], label='loss')\n",
    "   # plt.plot(history.history['val_loss'], label='val_loss')\n",
    "    plt.plot(history.history['accuracy'], label='accuracy')\n",
    "    plt.plot(history.history['val_accuracy'], label='val_accuracy')\n",
    "    plt.ylim([0, \n",
    "            max(\n",
    "                max(history.history['accuracy']),\n",
    "                max(history.history['val_accuracy'])\n",
    "            )])\n",
    "    plt.xlabel('Epoch')\n",
    "    plt.ylabel('Accuracy')\n",
    "    plt.title('Accuracy')\n",
    "    plt.legend()\n",
    "    plt.grid(True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "131a5b49-a808-4d41-ad4b-ae9469b994ea",
   "metadata": {},
   "source": [
    "**Data Pre-Processing**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f3693092",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Tile_ID</th>\n",
       "      <th>Long2</th>\n",
       "      <th>Lat2</th>\n",
       "      <th>Long1</th>\n",
       "      <th>Lat1</th>\n",
       "      <th>Mid_lat</th>\n",
       "      <th>Mid_long</th>\n",
       "      <th>Stop_Signs</th>\n",
       "      <th>Paving_historical</th>\n",
       "      <th>Paving_future</th>\n",
       "      <th>...</th>\n",
       "      <th>94129</th>\n",
       "      <th>94130</th>\n",
       "      <th>94131</th>\n",
       "      <th>94132</th>\n",
       "      <th>94133</th>\n",
       "      <th>94134</th>\n",
       "      <th>94141</th>\n",
       "      <th>94143</th>\n",
       "      <th>94158</th>\n",
       "      <th>94188</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>36</td>\n",
       "      <td>-122.514446</td>\n",
       "      <td>37.779636</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>37.779184</td>\n",
       "      <td>-122.513876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>37</td>\n",
       "      <td>-122.514446</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>37.778280</td>\n",
       "      <td>-122.513876</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>151</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.779636</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>37.779184</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>152</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.778732</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>37.778280</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>153</td>\n",
       "      <td>-122.513306</td>\n",
       "      <td>37.777829</td>\n",
       "      <td>-122.512166</td>\n",
       "      <td>37.776925</td>\n",
       "      <td>37.777377</td>\n",
       "      <td>-122.512736</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 53 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Tile_ID       Long2       Lat2       Long1       Lat1    Mid_lat  \\\n",
       "0       36 -122.514446  37.779636 -122.513306  37.778732  37.779184   \n",
       "1       37 -122.514446  37.778732 -122.513306  37.777829  37.778280   \n",
       "2      151 -122.513306  37.779636 -122.512166  37.778732  37.779184   \n",
       "3      152 -122.513306  37.778732 -122.512166  37.777829  37.778280   \n",
       "4      153 -122.513306  37.777829 -122.512166  37.776925  37.777377   \n",
       "\n",
       "     Mid_long  Stop_Signs  Paving_historical  Paving_future  ...  94129  \\\n",
       "0 -122.513876         0.0                0.0            0.0  ...      0   \n",
       "1 -122.513876         0.0                0.0            0.0  ...      0   \n",
       "2 -122.512736         0.0                0.0            0.0  ...      0   \n",
       "3 -122.512736         0.0                0.0            0.0  ...      0   \n",
       "4 -122.512736         0.0                0.0            0.0  ...      0   \n",
       "\n",
       "   94130  94131 94132  94133  94134  94141  94143  94158  94188  \n",
       "0      0      0     0      0      0      0      0      0      0  \n",
       "1      0      0     0      0      0      0      0      0      0  \n",
       "2      0      0     0      0      0      0      0      0      0  \n",
       "3      0      0     0      0      0      0      0      0      0  \n",
       "4      0      0     0      0      0      0      0      0      0  \n",
       "\n",
       "[5 rows x 53 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tiles = pd.read_csv(r'./Data/Tiles_binned_zipcode.csv')\n",
    "tiles.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c3a6b82c",
   "metadata": {},
   "source": [
    "Split into training and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "0d50fdd4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(tiles[['Tile_ID', 'Long2', 'Lat2', 'Long1', 'Lat1', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical', 'RTTYP_I',\n",
    "       'RTTYP_M', 'RTTYP_O', 'RTTYP_S', 'RTTYP_U', 'Collisions_Future_binary',\n",
    "       'Collisions_Historical_binary', 'bins_numeric', 'zip_code', '94101',\n",
    "       '94102', '94104', '94105', '94107', '94108', '94109', '94110', '94111',\n",
    "       '94112', '94114', '94115', '94116', '94117', '94118', '94121', '94122',\n",
    "       '94123', '94124', '94127', '94129', '94130', '94131', '94132', '94133',\n",
    "       '94134', '94141', '94143', '94158', '94188']], \n",
    "                                   tiles['bin'],\n",
    "                                   random_state=104, \n",
    "                                   test_size=0.20, \n",
    "                                   shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf220283",
   "metadata": {},
   "source": [
    "Image Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "9298e629",
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_PATH = './Satellite Imagery/Satellite Images Tiled/' \n",
    "\n",
    "def preprocess_data_part1(IMAGE_PATH):\n",
    "    \"\"\" Generate lists of images and labelsbased on temp_no_refer and temp_refer lists\n",
    "    \n",
    "    Params:\n",
    "    -------\n",
    "    IMAGE_PATH (str): path to directory with images.\n",
    "    \n",
    "    Returns:\n",
    "    --------\n",
    "    images_mini  (np.ndarray): Images of shape (N, 149 3)\n",
    "    \"\"\"\n",
    "    \n",
    "    data_mini = []\n",
    "    data_mini_test = []\n",
    "    for id in x_train['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]\n",
    "        \n",
    "        # append to images\n",
    "        data_mini.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini = np.stack(data_mini)\n",
    "    \n",
    "    for id in x_test['Tile_ID']:    \n",
    "                    \n",
    "        # read image and store as matrix            \n",
    "        # Index at the end makes all images the same size (they sometimes differ by 1 pixel)\n",
    "        image = tifffile.imread(IMAGE_PATH + str(id) + '.tif')[0:148, 0:188, :]\n",
    "        \n",
    "        # append to images\n",
    "        data_mini_test.append(image)\n",
    " \n",
    "    # stack images and trasnform to array\n",
    "    images_mini_test = np.stack(data_mini_test)\n",
    "    \n",
    "    return images_mini, images_mini_test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4d8f77ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "images_mini, images_mini_test = preprocess_data_part1(IMAGE_PATH)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5bd0d021-1c1d-4276-bff0-1f688168e3cd",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  (8376, 148, 188, 4)\n",
      "test  (2095, 148, 188, 4)\n"
     ]
    }
   ],
   "source": [
    "print('train ', np.shape(images_mini))\n",
    "print('test ', np.shape(images_mini_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57135842",
   "metadata": {},
   "source": [
    "Street Data Pre-Processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8dce0686",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train  (8376, 1, 1, 52)\n",
      "test  (2095, 1, 1, 52)\n"
     ]
    }
   ],
   "source": [
    "street = np.asarray(x_train[['Tile_ID', 'Long2', 'Lat2', 'Long1', 'Lat1', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical', 'RTTYP_I', 'RTTYP_M',\n",
    "       'RTTYP_O', 'RTTYP_S', 'RTTYP_U', 'Collisions_Future_binary',\n",
    "       'Collisions_Historical_binary', 'bins_numeric', 'zip_code', '94101',\n",
    "       '94102', '94104', '94105', '94107', '94108', '94109', '94110', '94111',\n",
    "       '94112', '94114', '94115', '94116', '94117', '94118', '94121', '94122',\n",
    "       '94123', '94124', '94127', '94129', '94130', '94131', '94132', '94133',\n",
    "       '94134', '94141', '94143', '94158', '94188']]).astype('float32')\n",
    "street_mini = []\n",
    "for row in range(len(street)):\n",
    "    street_mini.append([[street[row]]])\n",
    "street_mini = np.stack(street_mini)\n",
    "print('train ', np.shape(street_mini))\n",
    "\n",
    "street_test = np.asarray(x_test[['Tile_ID', 'Long2', 'Lat2', 'Long1', 'Lat1', 'Mid_lat', 'Mid_long',\n",
    "       'Stop_Signs', 'Paving_historical', 'Paving_future', 'Bus_stop',\n",
    "       'Collisions_Future', 'Collisions_Historical', 'RTTYP_I', 'RTTYP_M',\n",
    "       'RTTYP_O', 'RTTYP_S', 'RTTYP_U', 'Collisions_Future_binary',\n",
    "       'Collisions_Historical_binary', 'bins_numeric', 'zip_code', '94101',\n",
    "       '94102', '94104', '94105', '94107', '94108', '94109', '94110', '94111',\n",
    "       '94112', '94114', '94115', '94116', '94117', '94118', '94121', '94122',\n",
    "       '94123', '94124', '94127', '94129', '94130', '94131', '94132', '94133',\n",
    "       '94134', '94141', '94143', '94158', '94188']]).astype('float32')\n",
    "street_mini_test = []\n",
    "for row in range(len(street_test)):\n",
    "    street_mini_test.append([[street_test[row]]])\n",
    "street_mini_test = np.stack(street_mini_test)\n",
    "print('test ',np.shape(street_mini_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c5275800-edf8-4225-a07c-998b25870208",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8376, 111348)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_street = np.hstack(\n",
    "    (street_mini.reshape((8376,52)),\n",
    "     images_mini.reshape(8376,148*188*4))\n",
    ")\n",
    "np.shape(input_image_street)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "0771ebca-a17e-4a7f-b8b6-0ebbf258a123",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2095, 111348)"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "input_image_street_test = np.hstack(\n",
    "    (street_mini_test.reshape((2095,52)),\n",
    "     images_mini_test.reshape(2095,148*188*4))\n",
    ")\n",
    "np.shape(input_image_street_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7964c7a-cf51-4862-b868-402e6943589d",
   "metadata": {},
   "source": [
    "Pre-process Label Column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "a13e660c-d9ed-4991-ba6a-c0cece399b50",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8419     C\n",
       "10129    B\n",
       "7641     A\n",
       "5215     A\n",
       "7784     A\n",
       "Name: bin, dtype: object"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "7217a2b4-a4fb-4407-8f08-cf72e15cdbfa",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "le = preprocessing.LabelEncoder()\n",
    "le.fit(y_train)\n",
    "y_train = le.transform(y_train)\n",
    "le.fit(y_test)\n",
    "y_test = le.transform(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "2e009aa6-17b0-4b25-a570-aba1ce6100ae",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([2, 1, 0, 0, 0])"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_train[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "b77c3787-63c6-4417-a626-e29615e3ed26",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn import preprocessing\n",
    "# scaler = preprocessing.StandardScaler().fit(input_image_street)\n",
    "# X_scaled = scaler.transform(input_image_street)\n",
    "# X_scaled"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "94e4029c-9726-4105-bbb9-932ab1baad96",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# from sklearn.ensemble import RandomForestClassifier\n",
    "# from sklearn.model_selection import GridSearchCV\n",
    "# # Create the parameter grid based on the results of random search \n",
    "# param_grid = {\n",
    "#     'bootstrap': [True],'max_depth': [20,30,40, 100, 110],\n",
    "#     'max_features': ['sqrt'],'min_samples_leaf': [5,10,15],\n",
    "#     'min_samples_split': [40,50,60], 'n_estimators': [150, 200, 250]\n",
    "# }\n",
    "# # Create a based model\n",
    "# rf = RandomForestClassifier()\n",
    "# # Instantiate the grid search model\n",
    "# grid_search = GridSearchCV(estimator = rf, param_grid = param_grid, \n",
    "#                           cv = 3, n_jobs = -1, verbose = 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "bdaff37d-b624-434a-9b5c-0c20630e075d",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_search.fit(X_scaled, y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "9701c4cd-dfe9-4d74-a69b-b030194b187b",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# grid_search.best_params_"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6646a98-35b8-454d-9cb1-38e01d557b67",
   "metadata": {},
   "source": [
    "**Logistic Regression**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "99ae0d0a-47df-45ce-8bd4-9c31ed9d0cc3",
   "metadata": {},
   "source": [
    "**Modeling**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "1bd4861c-de6c-44cf-8590-9a7c382984bf",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0,  1,  2,  3,  4,  5,  6,  7,  8,  9, 10])"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0fba14bb-ba7e-4238-a864-98e2e6bc313d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0, 1, 2, 3, 4, 5, 6, 7, 8, 9])"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.unique(y_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "1c9257cc-c1de-40e8-9ea6-5a06399692e3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "class_weight = {0: 100,\n",
    "                1: 1000,\n",
    "                2: 1000,\n",
    "                3: 1000,\n",
    "                4: 1000,\n",
    "                5: 1000,\n",
    "                6: 1000,\n",
    "                7: 1000,\n",
    "                8: 1000,\n",
    "                9: 1000,\n",
    "                10: 1000,\n",
    "               }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "e178f9ab-ed0e-46ab-89af-482db92fd48b",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"logistic_model\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " input_1 (InputLayer)        [(None, 111348)]          0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 1)                 111349    \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 111,349\n",
      "Trainable params: 111,349\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 05:38:59.917241: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libcuda.so.1'; dlerror: libcuda.so.1: cannot open shared object file: No such file or directory\n",
      "2023-03-22 05:38:59.917275: W tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:265] failed call to cuInit: UNKNOWN ERROR (303)\n",
      "2023-03-22 05:38:59.917297: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:156] kernel driver does not appear to be running on this host (ip-172-31-13-91): /proc/driver/nvidia/version does not exist\n",
      "2023-03-22 05:38:59.917541: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/50\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2023-03-22 05:38:59.989442: W tensorflow/tsl/framework/cpu_allocator_impl.cc:82] Allocation of 2984126400 exceeds 10% of free system memory.\n",
      "2023-03-22 05:39:02.572883: W tensorflow/core/framework/op_kernel.cc:1830] OP_REQUIRES failed at sparse_xent_op.cc:102 : INVALID_ARGUMENT: Received a label value of 2 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0\n"
     ]
    },
    {
     "ename": "InvalidArgumentError",
     "evalue": "Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2469/1692682844.py\", line 21, in <module>\n      history = logistic_model.fit(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/backend.py\", line 5633, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 2 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_586]",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mInvalidArgumentError\u001b[0m                      Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[22], line 21\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m log_model\n\u001b[1;32m     19\u001b[0m logistic_model \u001b[38;5;241m=\u001b[39m create_logistic_model()\n\u001b[0;32m---> 21\u001b[0m history \u001b[38;5;241m=\u001b[39m \u001b[43mlogistic_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m     22\u001b[0m \u001b[43m    \u001b[49m\u001b[43minput_image_street\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     23\u001b[0m \u001b[43m    \u001b[49m\u001b[43my_train\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m     24\u001b[0m \u001b[43m    \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     25\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;66;43;03m# Calculate validation results on 20% of the training data.\u001b[39;49;00m\n\u001b[1;32m     26\u001b[0m \u001b[43m    \u001b[49m\u001b[43mvalidation_split\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m0.2\u001b[39;49m\u001b[43m,\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[43m    \u001b[49m\u001b[43mclass_weight\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mclass_weight\u001b[49m\n\u001b[1;32m     28\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/capstone/lib/python3.10/site-packages/keras/utils/traceback_utils.py:70\u001b[0m, in \u001b[0;36mfilter_traceback.<locals>.error_handler\u001b[0;34m(*args, **kwargs)\u001b[0m\n\u001b[1;32m     67\u001b[0m     filtered_tb \u001b[38;5;241m=\u001b[39m _process_traceback_frames(e\u001b[38;5;241m.\u001b[39m__traceback__)\n\u001b[1;32m     68\u001b[0m     \u001b[38;5;66;03m# To get the full stack trace, call:\u001b[39;00m\n\u001b[1;32m     69\u001b[0m     \u001b[38;5;66;03m# `tf.debugging.disable_traceback_filtering()`\u001b[39;00m\n\u001b[0;32m---> 70\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m e\u001b[38;5;241m.\u001b[39mwith_traceback(filtered_tb) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[1;32m     71\u001b[0m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[1;32m     72\u001b[0m     \u001b[38;5;28;01mdel\u001b[39;00m filtered_tb\n",
      "File \u001b[0;32m~/capstone/lib/python3.10/site-packages/tensorflow/python/eager/execute.py:52\u001b[0m, in \u001b[0;36mquick_execute\u001b[0;34m(op_name, num_outputs, inputs, attrs, ctx, name)\u001b[0m\n\u001b[1;32m     50\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m     51\u001b[0m   ctx\u001b[38;5;241m.\u001b[39mensure_initialized()\n\u001b[0;32m---> 52\u001b[0m   tensors \u001b[38;5;241m=\u001b[39m pywrap_tfe\u001b[38;5;241m.\u001b[39mTFE_Py_Execute(ctx\u001b[38;5;241m.\u001b[39m_handle, device_name, op_name,\n\u001b[1;32m     53\u001b[0m                                       inputs, attrs, num_outputs)\n\u001b[1;32m     54\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m core\u001b[38;5;241m.\u001b[39m_NotOkStatusException \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     55\u001b[0m   \u001b[38;5;28;01mif\u001b[39;00m name \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "\u001b[0;31mInvalidArgumentError\u001b[0m: Graph execution error:\n\nDetected at node 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits' defined at (most recent call last):\n    File \"/usr/lib/python3.10/runpy.py\", line 196, in _run_module_as_main\n      return _run_code(code, main_globals, None,\n    File \"/usr/lib/python3.10/runpy.py\", line 86, in _run_code\n      exec(code, run_globals)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel_launcher.py\", line 17, in <module>\n      app.launch_new_instance()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/traitlets/config/application.py\", line 1043, in launch_instance\n      app.start()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelapp.py\", line 725, in start\n      self.io_loop.start()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/tornado/platform/asyncio.py\", line 215, in start\n      self.asyncio_loop.run_forever()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 600, in run_forever\n      self._run_once()\n    File \"/usr/lib/python3.10/asyncio/base_events.py\", line 1896, in _run_once\n      handle._run()\n    File \"/usr/lib/python3.10/asyncio/events.py\", line 80, in _run\n      self._context.run(self._callback, *self._args)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 513, in dispatch_queue\n      await self.process_one()\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 502, in process_one\n      await dispatch(*args)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 409, in dispatch_shell\n      await result\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/kernelbase.py\", line 729, in execute_request\n      reply_content = await reply_content\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/ipkernel.py\", line 422, in do_execute\n      res = shell.run_cell(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/ipykernel/zmqshell.py\", line 540, in run_cell\n      return super().run_cell(*args, **kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 2961, in run_cell\n      result = self._run_cell(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3016, in _run_cell\n      result = runner(coro)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/async_helpers.py\", line 129, in _pseudo_sync_runner\n      coro.send(None)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3221, in run_cell_async\n      has_raised = await self.run_ast_nodes(code_ast.body, cell_name,\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3400, in run_ast_nodes\n      if await self.run_code(code, result, async_=asy):\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/IPython/core/interactiveshell.py\", line 3460, in run_code\n      exec(code_obj, self.user_global_ns, self.user_ns)\n    File \"/tmp/ipykernel_2469/1692682844.py\", line 21, in <module>\n      history = logistic_model.fit(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/utils/traceback_utils.py\", line 65, in error_handler\n      return fn(*args, **kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1650, in fit\n      tmp_logs = self.train_function(iterator)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1249, in train_function\n      return step_function(self, iterator)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1233, in step_function\n      outputs = model.distribute_strategy.run(run_step, args=(data,))\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1222, in run_step\n      outputs = model.train_step(data)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1024, in train_step\n      loss = self.compute_loss(x, y, y_pred, sample_weight)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/training.py\", line 1082, in compute_loss\n      return self.compiled_loss(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/engine/compile_utils.py\", line 265, in __call__\n      loss_value = loss_obj(y_t, y_p, sample_weight=sw)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 152, in __call__\n      losses = call_fn(y_true, y_pred)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 284, in call\n      return ag_fn(y_true, y_pred, **self._fn_kwargs)\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/losses.py\", line 2098, in sparse_categorical_crossentropy\n      return backend.sparse_categorical_crossentropy(\n    File \"/home/ubuntu/capstone/lib/python3.10/site-packages/keras/backend.py\", line 5633, in sparse_categorical_crossentropy\n      res = tf.nn.sparse_softmax_cross_entropy_with_logits(\nNode: 'sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits'\nReceived a label value of 2 which is outside the valid range of [0, 1).  Label values: 0 0 0 0 0 0 0 0 1 1 0 0 0 0 0 0 0 0 0 0 0 0 1 0 0 0 2 0 0 0 0 0\n\t [[{{node sparse_categorical_crossentropy/SparseSoftmaxCrossEntropyWithLogits/SparseSoftmaxCrossEntropyWithLogits}}]] [Op:__inference_train_function_586]"
     ]
    }
   ],
   "source": [
    "def create_logistic_model():\n",
    "     \n",
    "    input = tf.keras.layers.Input(shape=(111348,))\n",
    "\n",
    "    outputs = keras.layers.Dense(units = 1, activation = 'sigmoid')(input)\n",
    "\n",
    "    log_model = tf.keras.models.Model(inputs=input, outputs=outputs, name=\"logistic_model\")\n",
    "\n",
    "    log_model.summary()\n",
    "\n",
    "    log_model.compile(\n",
    "        optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "        loss='sparse_categorical_crossentropy',\n",
    "        metrics=['accuracy']\n",
    "        )\n",
    "    \n",
    "    return log_model\n",
    "\n",
    "logistic_model = create_logistic_model()\n",
    "\n",
    "history = logistic_model.fit(\n",
    "    input_image_street,\n",
    "    y_train,\n",
    "    epochs=50,\n",
    "    # Calculate validation results on 20% of the training data.\n",
    "    validation_split = 0.2,\n",
    "    class_weight = class_weight\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "35c8b5b1-4469-4d74-9883-099b8f529fc4",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "plot_loss(history)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
